{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# D209 Data Mining I Performance Assessment - Task 1\n",
    "### NVM2 — NVM2 Task 1: Classification Analysis\n",
    "#### Data Mining I — D209\n",
    "#### PRFA — NVM2\n",
    "> André Davis\n",
    "> StudentID: 010630641\n",
    "> MSDA\n",
    ">\n",
    "> Competencies\n",
    "> 4030.06.1 : Classification Data Mining Models\n",
    ">   The graduate applies observations to appropriate classes and categories using classification models.\n",
    ">\n",
    "> 4030.06.3 : Data Mining Model Performance\n",
    ">   The graduate evaluates data mining model performance for precision, accuracy, and model comparison.\n",
    "\n",
    "#### Table of Contents\n",
    "<ul>\n",
    "    <li><a href=\"#research-question\">A1: Research Question</li>\n",
    "    <li><a href=\"#objectives-and-goals\">A2: Goals of Analysis</a></li>\n",
    "    <li><a href=\"#justification\">B1: Justification of Classification Method</a></li>\n",
    "    <li><a href=\"#classification-model\">B2: Assumption of a Classification Model</a></li>\n",
    "    <li><a href=\"#packages-and-analysis-support\">B3: List Packages & Support of Analysis</a></li>\n",
    "    <li><a href=\"#data-preparation-goals\">C1: Data Preparation Goals and Necessary Manipulation</a></li>\n",
    "    <li><a href=\"#variable-selection-and-identification\">C2: Variable Selection \\& Identification</a></li>\n",
    "    <li><a href=\"#data-preparation\">C3: Preparation of Data</a></li>\n",
    "    <li><a href=\"#copy-of-prepared-data\">C4: Copy of Prepared Data Set</a></li>\n",
    "    <li><a href=\"#data-splitting-and-copying\">D1: Data Splitting, Copy of Split Data</a></li>\n",
    "    <li><a href=\"#analysis-description\">D2: Analysis Description</a></li>\n",
    "    <li><a href=\"#classification-analysis-code\">D3: Classification Analysis Code</a></li>\n",
    "    <li><a href=\"#accuracy-of-classification-model\">E1: Accuracy of Classification Model</a></li>\n",
    "    <li><a href=\"#model-results\">E2: Model Results</a></li>\n",
    "    <li><a href=\"#classification-limitations\">E3: Classification Limitations</a></li>\n",
    "    <li><a href=\"#recommended-action\">E4: Recommended Action</a></li>\n",
    "    <li><a href=\"#panopto-recording\">F: Panopto Recording</a></li>\n",
    "    <li><a href=\"#code-references\">G: Code References</a></li>\n",
    "    <li><a href=\"#source-references\">H: Source References</a></li>\n",
    "</ul>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"research-question\"></a>\n",
    "# A1: Research Question\n",
    "\n",
    "We will continue down the path that was set forth in the second task of D208, which used logical regression. The research question that will be focused of D209 Task 1 - Classification Analysis is `\"Which factors have a significant effect on readmission?\"`. The main goal of the classification analysis is to see if a more effective model can be created and give better details than D209 logistic regression model. The available options for this task are ['k-nearest neighbor (KNN)'](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) and ['Naive Bayes'](https://en.wikipedia.org/wiki/Naive_Bayes_classifier). I will be choosing *k-nearest neighbor (KNN)* to perform my classification analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"objectives-and-goals\"></a>\n",
    "# A2: Objective and Goals of Analysis\n",
    "\n",
    "At the end of Task 2 of D208 Logistical Regression a few explanatory variables presented themselves as things a hospital could look at or address in regard to readmission to the hospital. Even though the model did find some items for the hospital to work on we are going to use *k-nearest neighbor (KNN)* to see if the question can be answered more detailed via this model. The goal is attempting to create a better model for the question of readmitting patients is beneficial to the patient, hospital, and the laws in place to keep hospital readmission low. The hospital's core responsibility lies in handling existing issues and averting possible future ones. By proactively and efficiently managing these concerns, the hospital can improve outcomes for its present patients, avoid extending their hospital stay, prevent readmission for the same issue, and even avert potential complications or health risks that could lead to future hospitalization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"justification\"></a>\n",
    "# B1: Justification of Classification Method\n",
    "\n",
    "##### k-nearest neighbor (KNN)\n",
    "---\n",
    "\n",
    "The chosen method is *k-nearest neighbor (KNN)*. \"KNN is one of the simpler prediction/classification techniques: there is no model to be fit (as in regression). This doesn't mean that using KNN is an automatic procedure. The prediction results depend on how the features are scaled, how similarity is measured, and how big *K* is set. Also, all predictors must be in numeric form.\" (Bruce, Bruce, & Gedeck, 2019).\n",
    "\n",
    "KNN is a non-parametric algorithm that does not make explicit assumptions about the underlying distribution of the data. In KNN, the prediction for a new data point is based on the classification or value of regression of its nearest neighbors in the training data. The value of *K* represents the number of nearest neighbors considered. *K* will remain constant as new datapoints are being introduced.\n",
    "\n",
    "This method seems to be useful when working with the hospital data and questions as it allows new data points to be introduced and get classified to its proper section based on the surrounding data it lands by. This seems really flexible and a great way to classify out data appropriately.\n",
    "\n",
    "The biggest advantages of *KNN* include:\n",
    " - Quickest Calculation Time *(Tavva, 2020)*\n",
    " - Simple Algorithms *(Tavva, 2020)*\n",
    " - High Accuracy *(Tavva, 2020)*\n",
    " - Versatile – best use for Regression and Classification. *(Tavva, 2020)*\n",
    " - Doesn’t make any assumptions about data. *(Tavva, 2020)*\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"classification-model\"></a>\n",
    "# B2: Assumptions of a Classification Model\n",
    "\n",
    "##### K-Nearest Neighbor (KNN) Assumptions:\n",
    "\n",
    " * Homogeneous feature space:\n",
    "    - KNN assumes that the feature space is homogeneous, meaning that all features have the same scale and importance. If the features are not homogeneous, some features may be given more weight than others, leading to biased results. (Cover and Hart (1967))\n",
    "\n",
    " * The relevance of the distance metric:\n",
    "     * KNN relies heavily on the choice of distance metric used to measure similarity between instances. The algorithm assumes that the distance metric used is appropriate for the dataset and the problem being solved. Therefore, choosing an appropriate distance metric is critical to the performance of KNN. (Dasarathy (1991))\n",
    "\n",
    " * High dimensionality:\n",
    "     * KNN is sensitive to the curse of dimensionality, where the number of features is significantly higher than the number of instances. In high-dimensional spaces, the distance between the nearest neighbors becomes meaningless, making KNN less effective. (Aggarwal et al. (2001))\n",
    "\n",
    " * Imbalanced data:\n",
    "      * KNN assumes that the classes are equally distributed in the dataset. If the dataset is imbalanced, the algorithm may favor the majority class, leading to inaccurate results. (Batista et al. (2004))\n",
    "\n",
    " * Noisy data:\n",
    "     * KNN is susceptible to noisy data, which may introduce errors in the classification or regression process. To mitigate the effect of noisy data, preprocessing techniques such as outlier removal or noise reduction may be applied. (Wang and Wong (2008))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"packages-and-analysis-support\"></a>\n",
    "# B3: List Packages & Support of Analysis\n",
    "\n",
    "#### Analytic package for Python:\n",
    " * **Data manipulation:**\n",
    "    * [Pandas](https://pandas.pydata.org/docs/)\n",
    "      - Pandas library is used for data manipulation, analysis, and cleaning. Pandas' main data structures are the Series (1-dimensional) and DataFrame (2-dimensional).\n",
    "          - [Version 2](https://towardsdatascience.com/whats-new-in-pandas-2-0-5df366eb0197) is coming soon and will replace NumPy with Apache Arrows.\n",
    "    * [NumPy](https://numpy.org/doc/)\n",
    "         - Library used for numerical computing. It provides support for multidimensional arrays, matrices, and high-level mathematical functions to perform complex computations quickly and efficiently.\n",
    " * **Statistics:**\n",
    "    * [VIF - Variance Inflation Factor](https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html)\n",
    "        * *variance_inflation_factor* calculates the variance inflation factor (VIF) for a set of predictor variables in a linear regression model, which is a measure of the degree of multicollinearity between the predictors.\n",
    " * **Displaying Data:**\n",
    "    * [Seaborn](https://seaborn.pydata.org/)\n",
    "        * This is a data visualization library.\n",
    "    * [MatPlotLib](https://matplotlib.org/)\n",
    "        * This is a data visualization library.\n",
    " * **Analysis:**\n",
    "    * SkLearn (Scikit-learn - Machine Learning):\n",
    "        * [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "        * [preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "        * [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)\n",
    "        * [f_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html)\n",
    "        * [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "        * [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "        * [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "        * [roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)\n",
    "        * [roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)\n",
    "        * [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, classification_report\n",
    "\n",
    "\n",
    "\n",
    "#load the data ignoring the first column as it's simply an index\n",
    "medical_data = pd.read_csv('./Data/Medical/medical_clean.csv', index_col=0)\n",
    "medical_data.info()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"data-preparation-goals\"></a>\n",
    "# C1: Data Preparation Goals and Necessary Manipulation\n",
    " * D209 - *Data Mining I* is continuation of datasets from *D206 - Data Cleaning* and is given to us pre-cleaned to a point. Listed below are items pre-completed:\n",
    "     * Detecting Duplicates\n",
    "     * Dealing with missing values\n",
    "     * Detecting Outliers\n",
    " * Hot Encoding (*Dummy Variables* or *Indicator Variables*)\n",
    "     * \"Is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.\" (Vasudev, 2017).\n",
    "     * \"Binary 0-1 variables derived by recoding factor data for use in regression and other models\" (Bruce, P., Bruce, A., & Gedeck, P.  2019 p.163)\n",
    "     * If you have a Series that has 3 categorical levels you can split your series into two variables to represent the three states. If you have blue, yellow, and black and convert them to 1/0's then you only need two columns for the 3 series. For example if you have yellow = 0, and black = 0 then that means your value is blue without having that datapoint. We can apply this to the `Complication Risk` series in the dataset. It includes Low, Medium, and High. If we use the High (Series) and the Low (Series) it can be assumed if High and Low are both 0 values then the value represented is Medium.\n",
    "     * Pandas function for Hot-Encoding is `pd.get_dummies(data)` documentation is [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"variable-selection-and-identification\"></a>\n",
    "# C2: Variable Selection & Identification\n",
    "\n",
    "I am continuing on with the question from D208 so my variables are going to be reused to see if we can get better results with the *k-nearest neighbor (KNN)* model. Below are the variables that were used during logistic regression. Like before survey and census data was not relevant when looking for explanatory variables that were significant to readmission's (`ReAdmin`) to the hospital.\n",
    "\n",
    "| Data Type    | Variable                              |\n",
    "|--------------|---------------------------------------|\n",
    "| Continuous   | \tNumber of Children                   |\n",
    "| Continuous   | \tVitamin D levels                     |\n",
    "| Continuous   | \tNumber of Dr Visits                  |\n",
    "| Continuous   | \tNumber of Vitamin D supplements      |\n",
    "| Continuous   | \tFull meals                           |\n",
    "| Categorical  | Area (Rural, Suburban, Urban)         |\n",
    "| Categorical  | Gender (Male, Female, Nonbinary)      |\n",
    "| Categorical  | Readmission                           |\n",
    "| Categorical  | Soft Drink Consumption                |\n",
    "| Categorical  | Initial Admission Reason              |\n",
    "| Categorical  | High Blood Pressure                   |\n",
    "| Categorical  | Stroke                                |\n",
    "| Categorical  | Complication Risk (Low, Medium, High) |\n",
    "| Categorical  | Overweight                            |\n",
    "| Categorical  | Arthritis                             |\n",
    "| Categorical  | \tDiabetes                             |\n",
    "| Categorical  | \tHyperlipidemia                       |\n",
    "| Categorical  | \tAnxiety                              |\n",
    "| Categorical  | \tAllergic Rhinitis                    |\n",
    "| Categorical\t | Asthma                                |\n",
    "| Categorical\t | Days hospitalized                     |\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"data-preparation\"></a>\n",
    "# C3: Preparation of Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "original_medical = medical_data #storing backup of original data incase it's needed\n",
    "#del medical_clean_data[medical_clean_data.columns[0]]\n",
    "\n",
    "any_missing_values = medical_data.isna().values.any()\n",
    "if not any_missing_values:\n",
    "    print('Medical data does NOT contain any missing values\\n')\n",
    "else:\n",
    "    print('Medical data CONTAINS missing values.\\n')\n",
    "\n",
    "column_renames = {\n",
    "    'Item1': 'Timely_Admission'\n",
    "    ,'Item2': 'Timely_Treatment'\n",
    "    ,'Item3': 'Timely_Visits'\n",
    "    ,'Item4': 'Reliability'\n",
    "    ,'Item5': 'Options'\n",
    "    ,'Item6': 'Hours_Of_Treatment'\n",
    "    ,'Item7': 'Courteous_Staff'\n",
    "    ,'Item8': 'Listening' #Evidence of active listening from Doctor\n",
    "}\n",
    "medical_data.rename(columns=column_renames, inplace=True)\n",
    "#medical_clean_data\n",
    "\n",
    "removal_attributes = ['Customer_id', 'Interaction', 'UID',\n",
    "                      'Zip', 'Lat', 'Lng', 'City', 'State', 'County',\n",
    "                      'Area', 'Job', 'Marital', 'Population', 'TimeZone']\n",
    "\n",
    "medical_clean_data = medical_data.drop(columns=removal_attributes)\n",
    "\n",
    "category_dtype = 'category'\n",
    "convert_to_category = {\n",
    "    'Gender': category_dtype,\n",
    "    'ReAdmis': category_dtype,\n",
    "    'Soft_drink': category_dtype,\n",
    "    'Initial_admin': category_dtype,\n",
    "    'HighBlood': category_dtype,\n",
    "    'Stroke': category_dtype,\n",
    "    'Complication_risk': category_dtype,\n",
    "    'Overweight': category_dtype,\n",
    "    'Arthritis': category_dtype,\n",
    "    'Diabetes': category_dtype,\n",
    "    'Hyperlipidemia': category_dtype,\n",
    "    'BackPain': category_dtype,\n",
    "    'Anxiety': category_dtype,\n",
    "    'Allergic_rhinitis': category_dtype,\n",
    "    'Reflux_esophagitis': category_dtype,\n",
    "    'Asthma': category_dtype,\n",
    "    'Services': category_dtype,\n",
    "    'Timely_Admission': category_dtype,\n",
    "    'Timely_Treatment': category_dtype,\n",
    "    'Timely_Visits': category_dtype,\n",
    "    'Reliability': category_dtype,\n",
    "    'Options': category_dtype,\n",
    "    'Hours_Of_Treatment': category_dtype,\n",
    "    'Courteous_Staff': category_dtype,\n",
    "    'Listening': category_dtype\n",
    "}\n",
    "\n",
    "medical_clean_data = medical_clean_data.astype(convert_to_category)\n",
    "\n",
    "#Logical categorical variables converted to numerical\n",
    "columns_to_reexpress = ['ReAdmis', 'Soft_drink', 'HighBlood', 'Stroke',\n",
    "                        'Overweight', 'Arthritis', 'Diabetes', 'Hyperlipidemia',\n",
    "                        'BackPain', 'Anxiety', 'Allergic_rhinitis', 'Reflux_esophagitis',\n",
    "                        'Asthma']\n",
    "for column in columns_to_reexpress:\n",
    "    medical_clean_data[column] = medical_clean_data[column].map({'Yes': 1, 'No': 0 }).astype(np.int64)\n",
    "\n",
    "categorical_medical_data = medical_clean_data[convert_to_category.keys()]\n",
    "high_cardinalities = categorical_medical_data.nunique() > 3 #(> 3-5 Levels)\n",
    "high_cardinalities = high_cardinalities[high_cardinalities == True]\n",
    "high_cardinalities = list(high_cardinalities.index.values)\n",
    "print('Categoricals with high cardinality to be removed:')\n",
    "print(high_cardinalities)\n",
    "print('\\n')\n",
    "\n",
    "medical_clean_data = medical_clean_data.drop(columns=high_cardinalities)\n",
    "\n",
    "low_cardinalities = [item for item in list(convert_to_category.keys()) if item not in high_cardinalities]\n",
    "print('Categoricals remaining:')\n",
    "print(low_cardinalities)\n",
    "print('\\n')\n",
    "\n",
    "#Re-level ordinal/nominal categoricals\n",
    "complication_risk_dummies = pd.get_dummies(data=medical_clean_data['Complication_risk'], drop_first=True)\n",
    "\n",
    "medical_clean_data['complication_risk_medium'] = complication_risk_dummies['Medium']\n",
    "medical_clean_data['complication_risk_low'] = complication_risk_dummies['Low']\n",
    "\n",
    "initial_admission_dummies = pd.get_dummies(data=medical_clean_data['Initial_admin'], drop_first=True)\n",
    "\n",
    "medical_clean_data['initial_admission_emergency'] = initial_admission_dummies['Emergency Admission']\n",
    "medical_clean_data['initial_admission_observation'] = initial_admission_dummies['Observation Admission']\n",
    "\n",
    "gender_dummies = pd.get_dummies(data=medical_clean_data['Gender'], drop_first=True)\n",
    "\n",
    "medical_clean_data['gender_male'] = gender_dummies['Male']\n",
    "medical_clean_data['gender_non_binary'] = gender_dummies['Nonbinary']\n",
    "\n",
    "regression_variables = ['Children', 'Age', 'VitD_levels', 'HighBlood', 'Overweight', 'Arthritis', 'Diabetes', 'BackPain', 'Asthma', 'Initial_days', 'ReAdmis', 'complication_risk_medium', 'complication_risk_low', 'initial_admission_emergency', 'initial_admission_observation', 'gender_male','gender_non_binary']\n",
    "\n",
    "prepared_medical_data = medical_clean_data[regression_variables]\n",
    "\n",
    "print('Review Each Data Series\\n')\n",
    "for column in prepared_medical_data.columns:\n",
    "    current = prepared_medical_data[column]\n",
    "    if np.issubdtype(current.dtype, np.number):\n",
    "        print(f'{current.describe()}\\n')\n",
    "    else:\n",
    "        print(f'{current.value_counts()}\\n')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"copy-of-prepared-data\"></a>\n",
    "# C4: Copy of Prepared Data Set\n",
    "\n",
    "##### Preparation Libraries Assistance:\n",
    " * To aid the in the variable selection process we will be using [`SelectKBest`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)\n",
    " * To check for Multicollinearity we will use the library [`variance_inflation_factor`](https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "copy_of_prepared_medical_data = prepared_medical_data.copy()\n",
    "copy_of_prepared_medical_data.to_csv('./prepared-medical-data-for-feature-selection.csv')\n",
    "\n",
    "dependent_variable = prepared_medical_data['ReAdmis']\n",
    "explanatory_variables = prepared_medical_data.drop(columns=['ReAdmis'])\n",
    "\n",
    "explanatory_variables = pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(explanatory_variables), columns=explanatory_variables.columns)\n",
    "\n",
    "#print(dependent_variable.info())\n",
    "#print(explanatory_variables.info())\n",
    "\n",
    "#We'll take our explanatory variables and run SelectKBest function on them to select the best features for the model based against the dependent variable\n",
    "feature_selection = SelectKBest(f_classif, k='all')\n",
    "feature_selection.fit(explanatory_variables, dependent_variable)\n",
    "\n",
    "feature_column = 'Feature'\n",
    "p_value_column = 'p_values'\n",
    "\n",
    "f_pvalues = { feature_column: explanatory_variables.columns, p_value_column: feature_selection.pvalues_ }\n",
    "feature_selection_pvalues = pd.DataFrame(f_pvalues)\n",
    "feature_selection_pvalues = feature_selection_pvalues.sort_values(p_value_column)\n",
    "\n",
    "kept_features = feature_selection_pvalues[feature_selection_pvalues[p_value_column] < .05]\n",
    "\n",
    "print(kept_features.info())\n",
    "print(kept_features)\n",
    "\n",
    "selected_features_columns = list(kept_features[feature_column])\n",
    "print(selected_features_columns)\n",
    "\n",
    "prepared_medical_data[selected_features_columns].to_csv('./feature-selected-medical-data.csv')\n",
    "\n",
    "# Of the selected features, we are going to check them for `multicollinearity`\n",
    "# We don't want any highly correlated dependent variables as this would cause redundency within the model.\n",
    "multicollinearity_features_to_check = prepared_medical_data[selected_features_columns]\n",
    "\n",
    "\n",
    "vif_data = {\n",
    "    'feature': multicollinearity_features_to_check.columns,\n",
    "    'VIF': [variance_inflation_factor(multicollinearity_features_to_check.values, i) for i in range(len(multicollinearity_features_to_check.columns))]\n",
    "}\n",
    "vif_features = pd.DataFrame(vif_data)\n",
    "print(vif_features)\n",
    "\n",
    "'''\n",
    "    VIF Rules:\n",
    "        - VIF values of 1 are considered correlated\n",
    "        - VIF values below 1 are not correlated\n",
    "        - VIF values greater than 5 are highly correlated and should be reduced to avoid redundancy\n",
    "'''\n",
    "highly_correlated_features = vif_features[vif_features['VIF'] >= 5.0]\n",
    "\n",
    "#Confirm that we have no `multicollinearity`\n",
    "try:\n",
    "    assert (len(highly_correlated_features) == 0), 'No features should have an VIF of 5 or greater.'\n",
    "except AssertionError as exception:\n",
    "    print('High multicollinearity found.', exception)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"data-splitting-and-copying\"></a>\n",
    "# D1: Data Splitting, Copy of Split Data\n",
    "\n",
    "Get Training and Test data for `features` and `dependent variable` using train_test_split(...) function.\n",
    "\n",
    "To make sure that the linear regression aspect using the correct value for intercept we will use the [`add_constant()`](https://www.statsmodels.org/stable/generated/statsmodels.tools.tools.add_constant.html)\n",
    "\n",
    "Training and Test data will be split into 80% training and 20% test data for explanatory variables and dependent variable\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_explanatory = sm.add_constant(explanatory_variables[selected_features_columns])\n",
    "y_target = dependent_variable\n",
    "\n",
    "print(X_explanatory.shape)\n",
    "print(y_target.shape)\n",
    "\n",
    "X_explanatory_training, X_explanatory_test, y_target_training, y_target_testing = train_test_split(X_explanatory, y_target, test_size=.2, random_state=42, stratify=y_target)\n",
    "\n",
    "#Save training and test datas\n",
    "X_explanatory_training.to_csv('./x_explanatory-training-dataset.csv')\n",
    "X_explanatory_test.to_csv('./x_explanatory-testing-dataset.csv')\n",
    "\n",
    "y_target_training.to_csv('./y_target-training-dataset.csv')\n",
    "y_target_testing.to_csv('./y_target-testing-dataset.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"analysis-description\"></a>\n",
    "# D2: Analysis Description\n",
    "\n",
    "K-nearest Neighbors Classification requires that we pick an appropriate value for *k*. We can use the function [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) from scikit-learn that performs a grid search over a specified hyperparameter space to find the optimal combination of hyperparameters for a given machine learning model.\n",
    "\n",
    "We will visualize our results with Receiver Operating Characteristic (ROC) Curve. This curve is the visual relationship between the *true positive rate (TPR)* and * false positive rate (FPR)* for various thresholds.\n",
    "\n",
    "After the ROC Curve is created we can find the *Area Under the Curve (AUC)* to get a score of how it will work with the classification model.\n",
    "  > AUC is simply the total area under the ROC curve. The larger the value of AUC, the more effective the classifier.\n",
    "  > An AUC of 1 indicates a perfect classifier:\n",
    "  >     If gets all the 1s correctly classified, and it doesn't misclassify any 0s as 1s.\n",
    "  > (Bruce, P., Bruce, A., & Gedeck, P.  2019 p.226)\n",
    "\n",
    "Tools being used for ROC and AUC:\n",
    " * [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    " * [`roc_curve`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)\n",
    " * [`roc_auc_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)\n",
    " * [`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate best value for 'k'\n",
    "param_grid = { 'n_neighbors': np.arange(1, 50) }\n",
    "knn_cross_validation = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
    "knn_cross_validation.fit(X_explanatory_training, y_target_training)\n",
    "\n",
    "n_neighbors = knn_cross_validation.best_params_['n_neighbors']\n",
    "\n",
    "print(f'Best Parameter from KNN Cross-Validation: {n_neighbors}')\n",
    "print(f'Best Score from KNN Cross-Validation: {knn_cross_validation.best_score_}')\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "knn.fit(X_explanatory_training, y_target_training)\n",
    "\n",
    "#use the test data to make some predictions for our Target(Dependent) Variable\n",
    "y_target_predictions = knn.predict(X_explanatory_test)\n",
    "\n",
    "#Get values to evaluate the performance of a classification model\n",
    "true_negative, false_positive, false_negative, true_positive = confusion_matrix(y_target_testing, y_target_predictions).ravel()\n",
    "\n",
    "c_matrix = f'''| {' ':22} | {'Predicted: No Readmission':^5} | {'Predicted: Readmission':^5} |\\n\n",
    "        |{'':->24}|{'':->27}|{'':->24}|\\n\n",
    "        | {'Actual: No Readmission':^5} | {true_negative:^25} | {false_positive:^22} |\\n\n",
    "        | {'Actual: Readmission':22} | {false_negative:^25} | {true_positive:^22} |\\n\n",
    "        |{'':->24}|{'':->27}|{'':->24}|\\n'''\n",
    "\n",
    "display_confusion_matrix = textwrap.fill(c_matrix, width=80)\n",
    "print('\\n')\n",
    "print(display_confusion_matrix)\n",
    "print('\\n')\n",
    "print(f'KNN Classification Scores:')\n",
    "print(f'\\tTraining accuracy: [{knn.score(X_explanatory_training, y_target_training)}]')\n",
    "print(f'\\tTesting accuracy: [{knn.score(X_explanatory_test, y_target_testing)}]')\n",
    "\n",
    "# Plotting ROC\n",
    "y_target_prediction_probability = knn.predict_proba(X_explanatory_test)[:,1]\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_target_testing, y_target_prediction_probability)\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.plot(false_positive_rate, true_positive_rate, label='K-Nearest Neighbor(KNN)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('K-Nearest Neighbor(KNN) ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "rocc_auc_score = roc_auc_score(y_target_testing, y_target_prediction_probability)\n",
    "print(f'AUC (Area Under the Curve) Score: {rocc_auc_score}')\n",
    "\n",
    "report = classification_report(y_target_testing, y_target_predictions)\n",
    "print(report)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"classification-analysis-code\"></a>\n",
    "# D3: Classification Analysis Code\n",
    "\n",
    "Because this whole assignment was done in a Notebook the code for the Analysis was created and run in Sections D1 & D2. As such it will be displayed below as Markdown as to not have the code run multiple times in the notebook.\n",
    "\n",
    "###### D1 sections code:\n",
    "\n",
    "```python\n",
    "X_explanatory = sm.add_constant(explanatory_variables[selected_features_columns])\n",
    "y_target = dependent_variable\n",
    "\n",
    "print(X_explanatory.shape)\n",
    "print(y_target.shape)\n",
    "\n",
    "X_explanatory_training, X_explanatory_test, y_target_training, y_target_testing = train_test_split(X_explanatory, y_target, test_size=.2, random_state=42, stratify=y_target)\n",
    "\n",
    "#Save training and test datas\n",
    "X_explanatory_training.to_csv('./x_explanatory-training-dataset.csv')\n",
    "X_explanatory_test.to_csv('./x_explanatory-testing-dataset.csv')\n",
    "\n",
    "y_target_training.to_csv('./y_target-training-dataset.csv')\n",
    "y_target_testing.to_csv('./y_target-testing-dataset.csv')\n",
    "```\n",
    "\n",
    "##### D2 sections code:\n",
    "\n",
    "```python\n",
    "# Calculate best value for 'k'\n",
    "param_grid = { 'n_neighbors': np.arange(1, 50) }\n",
    "knn_cross_validation = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
    "knn_cross_validation.fit(X_explanatory_training, y_target_training)\n",
    "\n",
    "n_neighbors = knn_cross_validation.best_params_['n_neighbors']\n",
    "\n",
    "print(f'Best Parameter from KNN Cross-Validation: {n_neighbors}')\n",
    "print(f'Best Score from KNN Cross-Validation: {knn_cross_validation.best_score_}')\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "knn.fit(X_explanatory_training, y_target_training)\n",
    "\n",
    "#use the test data to make some predictions for our Target(Dependent) Variable\n",
    "y_target_predictions = knn.predict(X_explanatory_test)\n",
    "\n",
    "#Get values to evaluate the performance of a classification model\n",
    "true_negative, false_positive, false_negative, true_positive = confusion_matrix(y_target_testing, y_target_predictions).ravel()\n",
    "\n",
    "c_matrix = f'''| {' ':22} | {'Predicted: No Readmission':^5} | {'Predicted: Readmission':^5} |\\n\n",
    "        |{'':->24}|{'':->27}|{'':->24}|\\n\n",
    "        | {'Actual: No Readmission':^5} | {true_negative:^25} | {false_positive:^22} |\\n\n",
    "        | {'Actual: Readmission':22} | {false_negative:^25} | {true_positive:^22} |\\n\n",
    "        |{'':->24}|{'':->27}|{'':->24}|\\n'''\n",
    "\n",
    "display_confusion_matrix = textwrap.fill(c_matrix, width=80)\n",
    "print('\\n')\n",
    "print(display_confusion_matrix)\n",
    "print('\\n')\n",
    "print(f'KNN Classification Scores:')\n",
    "print(f'\\tTraining accuracy: [{knn.score(X_explanatory_training, y_target_training)}]')\n",
    "print(f'\\tTesting accuracy: [{knn.score(X_explanatory_test, y_target_testing)}]')\n",
    "\n",
    "# Plotting ROC\n",
    "y_target_prediction_probability = knn.predict_proba(X_explanatory_test)[:,1]\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_target_testing, y_target_prediction_probability)\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.plot(false_positive_rate, true_positive_rate, label='K-Nearest Neighbor(KNN)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('K-Nearest Neighbor(KNN) ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "rocc_auc_score = roc_auc_score(y_target_testing, y_target_prediction_probability)\n",
    "print(f'AUC (Area Under the Curve) Score: {rocc_auc_score}')\n",
    "\n",
    "report = classification_report(y_target_testing, y_target_predictions)\n",
    "print(report)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"accuracy-of-classification-model\"></a>\n",
    "# E1: Accuracy of Classification Model\n",
    "\n",
    "In Task 2 of D209 the reduced logistical regression model ended up with an accuracy of *0.98 (98%)* with a MSE of *0.017 (1.7%)* with regard to factors that lead to hospital readmission's. This meant that the model had a high-level of accuracy and the MSE confirmed that the accuracy wasn't inaccurately describing its accuracy as the MSE was really low. When working through D209 Task 1 classification analysis I wanted to use the same question and see if k-nearest neighbor(KNN) could top or at a minimum maintain the accuracy for readmission's modeling.\n",
    "\n",
    "The classification model k-nearest neighbors (KNN) had pretty high accuracy for both the `training` and `test` data. Those accuracies were *0.98 (98%)* for the training dataset and *0.97 (97%)* for testing dataset with regard to which factors affect hospitals' readmission. The point of the *ROC Curve* is to see how far away from the diagonal line we can get in the positive direction. This is because the diagonal line represents the 50% correct classification rate, where complete randomness would hover around this line. If the classification line falls below the diagonal line, it's a poor model, and if it goes above the diagonal line, it's a better model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"model-results\"></a>\n",
    "# E2: Model Results\n",
    "\n",
    "The `KNN (k-nearest neighbor)` model had a remarkable `AUC (Area Under the Curve)` coming in at nearly 100% (*0.9985671147728692*.) This model is a very accurate one when it comes to predicting re-admissions to the hospital. The KNN model almost couldn't be any more accurate. This is on par with the accuracy of the `logistical regression` model performed in *D208*. This model will be extremely helpful in predicting if a patient is likely to be readmitted to the hospital or not as it has reduced the features of the model to the most important ones via the `SelectKBest` functionality."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"classification-limitations\"></a>\n",
    "# E3: Classification Limitations\n",
    "\n",
    "k-nearest neighbor (KNN) limitations:\n",
    "  - Dimensionaility issues:\n",
    "    - KNN is sensitive to too many dimensions/features. Because it calculates the distance between the nearest neighbor and increased feature/dimensions reduces density it highly affects the model.\n",
    "  - Complexity Computational Issue:\n",
    "    - Because every instance has to be calculated, larger datasets can be computationally expensive.\n",
    "  - Imbalanced data:\n",
    "    - If a class is way too big, it may dominate the nearest neighbors causing poor classifications.\n",
    "  - Finding the optimal value of k:\n",
    "    -   Inappropriate k values can lead to poor classifications and to find the right one can simply be trial and error\n",
    "  - Outliers:\n",
    "    - Outliers can affect the distances between instances throwing off the calculations\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"recommended-action\"></a>\n",
    "# E4: Recommended Action\n",
    "\n",
    "In D208, the logistical regression was pretty accurate and reduced down to nine features. During Task 1 of D209 we used *KNN* to attempt to create a better model. The model was reduced down to the three most correlated features. Those features were `Initial_days`, `Children`, and `initial_admission_emergency`.\n",
    "\n",
    "Recommendations include:\n",
    "\n",
    "Enhancing discharge planning is critical in reducing readmission rates, especially for patients with multiple children and those initially admitted through emergency departments. Research indicates that the length of the first hospitalization significantly affects the likelihood of future readmission's. As such, hospitals should focus on improving their discharge planning procedures to ensure patients receive adequate preparation to manage their conditions at home. This may entail providing clear guidance on medication administration, follow-up visits, and identifying warning signs of potential complications. By improving these processes, hospitals can equip patients and their families with the necessary resources to effectively manage their health outside the hospital and ultimately reduce readmission rates, particularly for patients with multiple children and emergency department admissions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(selected_features_columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"panopto-recording\"></a>\n",
    "# F: Panopto Recording\n",
    "\n",
    "Summary of Environments:\n",
    "  * OS: Windows 11 + macOS Ventura (I work in both environments)\n",
    "  * Language: Python\n",
    "  * Environment: Jupyter Notebook through JetBrains DataSpell IDE (Cross-Platform)\n",
    "\n",
    "[D209 - Panopto Recording](https://wgu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=89926e14-d13d-4982-a945-afe90061618a)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"code-references\"></a>\n",
    "# G: Code References\n",
    "\n",
    "[Dr. Elleh Webinar - D209 T1 Jul 12 2022](https://wgu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=b73b6274-ef01-4d1b-a59f-aed100228a93)\n",
    " * Used for:\n",
    "     * D1: Splitting Data\n",
    "\n",
    "[Dr. Elleh Webinar - D209 Task 1 KNN Python Code Demo Sep 11 2022](https://wgu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=22e0a080-84f3-4978-a853-af16011c6ce3)\n",
    " * Used for:\n",
    "     * C3: SelectKBest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"source-references\"></a>\n",
    "# H: Source References\n",
    "\n",
    "#### Citations\n",
    " * Bruce, P., Bruce, A., & Gedeck, P. (2019). Practical Statistics for Data Scientists: 50 Essential Concepts. O'Reilly Media, Inc.\n",
    " * Tavva, R. (2020, October 10). K Nearest Neighbour For Supervised Learning. Data Science Blog. [https://data-science-blog.com/blog/2020/10/10/k-nearest-neighbour-for-supervised-learning/](https://data-science-blog.com/blog/2020/10/10/k-nearest-neighbour-for-supervised-learning/)\n",
    " * Cover, T., & Hart, P. (1967). Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13(1), 21-27. Retrieved from [https://ieeexplore.ieee.org/document/1054010](https://ieeexplore.ieee.org/document/1054010)\n",
    " * Dasarathy, B. V. (1991). Nearest neighbor (NN) norms: NN pattern classification techniques. IEEE Computer Society Press. Retrieved from [https://ieeexplore.ieee.org/document/134048](https://ieeexplore.ieee.org/document/134048)\n",
    " * Aggarwal, C. C., Hinneburg, A., & Keim, D. A. (2001). On the surprising behavior of distance metrics in high dimensional space. International Conference on Database Theory, 420-434. Retrieved from [https://link.springer.com/chapter/10.1007/3-540-44503-X_25](https://link.springer.com/chapter/10.1007/3-540-44503-X_25)\n",
    " * Citation: Batista, G. E., Prati, R. C., & Monard, M. C. (2004). A study of the behavior of several methods for balancing machine learning training data. ACM SIGKDD Explorations Newsletter, 6(1), 20-29. Retrieved from [https://dl.acm.org/doi/10.1145/1007730.1007735](https://dl.acm.org/doi/10.1145/1007730.1007735)\n",
    " * Wang, Y., & Wong, K. C. (2008). Enhancing K-nearest neighbor classifier with local information. International Journal of Pattern Recognition and Artificial Intelligence, 22(01), 135-154. Retrieved from [https://www.worldscientific.com/doi/abs/10.1142/S0218001408006325](https://www.worldscientific.com/doi/abs/10.1142/S0218001408006325)\n",
    "  * Vasudev. (2017). What is One Hot Encoding? Why and When Do You Have to Use it? HackerNoon. [https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
