{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# D209 Data Mining I Performance Assessment - Task 1\n",
    "### NVM2 — NVM2 Task 1: Classification Analysis\n",
    "#### Data Mining I — D209\n",
    "#### PRFA — NVM2\n",
    "> André Davis\n",
    "> StudentID: 010630641\n",
    "> MSDA\n",
    ">\n",
    "> Competencies\n",
    "> 4030.06.1 : Classification Data Mining Models\n",
    ">   The graduate applies observations to appropriate classes and categories using classification models.\n",
    "> 4030.06.3 : Data Mining Model Performance\n",
    ">   The graduate evaluates data mining model performance for precision, accuracy, and model comparison.\n",
    "\n",
    "#### Table of Contents\n",
    "<ul>\n",
    "    <li><a href=\"#research-question\">A1: Research Question</li>\n",
    "    <li><a href=\"#objectives-and-goals\">A2: Goals of Analysis</a></li>\n",
    "    <li><a href=\"#justification\">B1: Justification of Classification Method</a></li>\n",
    "    <li><a href=\"#classification-model\">B2: Assumption of a Classification Model</a></li>\n",
    "    <li><a href=\"#packages-and-analysis-support\">B3: List Packages & Support of Analysis</a></li>\n",
    "    <li><a href=\"#data-preparation-goals\">C1: Data Preparation Goals and Necessary Manipulation</a></li>\n",
    "    <li><a href=\"#variable-selection-and-identification\">C2: Variable Selection \\& Identification</a></li>\n",
    "    <li><a href=\"#data-preparation\">C3: Preparation of Data</a></li>\n",
    "    <li><a href=\"#copy-of-prepared-data\">C4: Copy of Prepared Data Set</a></li>\n",
    "    <li><a href=\"#data-splitting-and-copying\">D1: Data Splitting, Copy of Split Data</a></li>\n",
    "    <li><a href=\"#analysis-description\">D2: Analysis Description</a></li>\n",
    "    <li><a href=\"#classification-analysis-code\">D3: Classification Analysis Code</a></li>\n",
    "    <li><a href=\"#accuracy-of-classification-model\">E1: Accuracy of Classification Model</a></li>\n",
    "    <li><a href=\"#model-results\">E2: Model Results</a></li>\n",
    "    <li><a href=\"#classification-limitations\">E3: Classification Limitations</a></li>\n",
    "    <li><a href=\"#recommended-action\">E4: Recommended Action</a></li>\n",
    "    <li><a href=\"#panopto-recording\">F: Panopto Recording</a></li>\n",
    "    <li><a href=\"#code-references\">G: Code References</a></li>\n",
    "    <li><a href=\"#source-references\">H: Source References</a></li>\n",
    "</ul>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"research-question\"></a>\n",
    "# A1: Research Question\n",
    "\n",
    "We will continue down the path that was set forth in the second task of D208, which used logical regression. The research question that will be focused of D209 Task 1 - Classification Analysis is \"Which factors have a significant effect on readmission?\". The main goal of the classification analysis is to see if a more effective model can be created and give better details than D209 logistic regression model. The available options for this task are ['k-nearest neighbor (KNN)'](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) and ['Naive Bayes'](https://en.wikipedia.org/wiki/Naive_Bayes_classifier). I will be choosing *k-nearest neighbor (KNN)* to perform my classification analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"objectives-and-goals\"></a>\n",
    "# A2: Objective and Goals of Analysis\n",
    "\n",
    "At the end of Task 2 of D208 Logistical Regression a few explanatory variables presented themselves as things a hospital could look at or address in regard to readmission to the hospital. Even though the model did find some items for the hospital to work on we are going to use *k-nearest neighbor (KNN)* to see if the question can be answered more detailed via this model. The goal is attempting to create a better model for the question of readmitting patients is beneficial to the patient, hospital, and the laws in place to keep hospital readmission low. The hospital's core responsibility lies in handling existing issues and averting possible future ones. By proactively and efficiently managing these concerns, the hospital can improve outcomes for its present patients, avoid extending their hospital stay, prevent readmission for the same issue, and even avert potential complications or health risks that could lead to future hospitalization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"justification\"></a>\n",
    "# B1: Justification of Classification Method\n",
    "\n",
    "##### k-nearest neighbor (KNN)\n",
    "---\n",
    "\n",
    "The chosen method is *k-nearest neighbor (KNN)*. \"KNN is one of the simpler prediction/classification techniques: there is no model to be fit (as in regression). This doesn't mean that using KNN is an automatic procedure. The prediction results depend on how the features are scaled, how similarity is measured, and how big *K* is set. Also, all predictors must be in numeric form.\" (Bruce, Bruce, & Gedeck, 2019).\n",
    "\n",
    "KNN is a non-parametric algorithm that does not make explicit assumptions about the underlying distribution of the data. In KNN, the prediction for a new data point is based on the classification or value of regression of its nearest neighbors in the training data. The value of *K* represents the number of nearest neighbors considered. *K* will remain constant as new datapoints are being introduced.\n",
    "\n",
    "This method seems to be useful when working with the hospital data and questions as it allows new data points to be introduced and get classified to its proper section based on the surrounding data it lands by. This seems really flexible and a great way to classify out data appropriately.\n",
    "\n",
    "The biggest advantages of *KNN* include:\n",
    " - Quickest Calculation Time *(Tavva, 2020)*\n",
    " - Simple Algorithms *(Tavva, 2020)*\n",
    " - High Accuracy *(Tavva, 2020)*\n",
    " - Versatile – best use for Regression and Classification. *(Tavva, 2020)*\n",
    " - Doesn’t make any assumptions about data. *(Tavva, 2020)*\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"classification-model\"></a>\n",
    "# B2: Assumptions of a Classification Model\n",
    "\n",
    "##### K-Nearest Neighbor (KNN) Assumptions:\n",
    "\n",
    " * Homogeneous feature space:\n",
    "    - KNN assumes that the feature space is homogeneous, meaning that all features have the same scale and importance. If the features are not homogeneous, some features may be given more weight than others, leading to biased results. (Cover and Hart (1967))\n",
    "\n",
    " * The relevance of the distance metric:\n",
    "     * KNN relies heavily on the choice of distance metric used to measure similarity between instances. The algorithm assumes that the distance metric used is appropriate for the dataset and the problem being solved. Therefore, choosing an appropriate distance metric is critical to the performance of KNN. (Dasarathy (1991))\n",
    "\n",
    " * High dimensionality:\n",
    "     * KNN is sensitive to the curse of dimensionality, where the number of features is significantly higher than the number of instances. In high-dimensional spaces, the distance between the nearest neighbors becomes meaningless, making KNN less effective. (Aggarwal et al. (2001))\n",
    "\n",
    " * Imbalanced data:\n",
    "      * KNN assumes that the classes are equally distributed in the dataset. If the dataset is imbalanced, the algorithm may favor the majority class, leading to inaccurate results. (Batista et al. (2004))\n",
    "\n",
    " * Noisy data:\n",
    "     * KNN is susceptible to noisy data, which may introduce errors in the classification or regression process. To mitigate the effect of noisy data, preprocessing techniques such as outlier removal or noise reduction may be applied. (Wang and Wong (2008))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"packages-and-analysis-support\"></a>\n",
    "# B3: List Packages & Support of Analysis\n",
    "\n",
    "#### Analytic package for Python:\n",
    " * **Data manipulation:**\n",
    "    * [Pandas](https://pandas.pydata.org/docs/)\n",
    "      - Pandas library is used for data manipulation, analysis, and cleaning. Pandas' main data structures are the Series (1-dimensional) and DataFrame (2-dimensional).\n",
    "          - [Version 2](https://towardsdatascience.com/whats-new-in-pandas-2-0-5df366eb0197) is coming soon and will replace NumPy with Apache Arrows.\n",
    "    * [NumPy](https://numpy.org/doc/)\n",
    "         - Library used for numerical computing. It provides support for multidimensional arrays, matrices, and high-level mathematical functions to perform complex computations quickly and efficiently.\n",
    " * **Statistics:**\n",
    "    * [VIF - Variance Inflation Factor](https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html)\n",
    "        * *variance_inflation_factor* calculates the variance inflation factor (VIF) for a set of predictor variables in a linear regression model, which is a measure of the degree of multicollinearity between the predictors.\n",
    " * **Displaying Data:**\n",
    "    * [Seaborn](https://seaborn.pydata.org/)\n",
    "        * This is a data visualization library.\n",
    "    * [MatPlotLib](https://matplotlib.org/)\n",
    "        * This is a data visualization library.\n",
    " * **Analysis:**\n",
    "    * SkLearn (Scikit-learn - Machine Learning):\n",
    "        * [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "        * [preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "        * [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)\n",
    "        * [f_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html)\n",
    "        * [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "        * [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "        * [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "        * [roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)\n",
    "        * [roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)\n",
    "        * [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10000 entries, 1 to 10000\n",
      "Data columns (total 49 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Customer_id         10000 non-null  object \n",
      " 1   Interaction         10000 non-null  object \n",
      " 2   UID                 10000 non-null  object \n",
      " 3   City                10000 non-null  object \n",
      " 4   State               10000 non-null  object \n",
      " 5   County              10000 non-null  object \n",
      " 6   Zip                 10000 non-null  int64  \n",
      " 7   Lat                 10000 non-null  float64\n",
      " 8   Lng                 10000 non-null  float64\n",
      " 9   Population          10000 non-null  int64  \n",
      " 10  Area                10000 non-null  object \n",
      " 11  TimeZone            10000 non-null  object \n",
      " 12  Job                 10000 non-null  object \n",
      " 13  Children            10000 non-null  int64  \n",
      " 14  Age                 10000 non-null  int64  \n",
      " 15  Income              10000 non-null  float64\n",
      " 16  Marital             10000 non-null  object \n",
      " 17  Gender              10000 non-null  object \n",
      " 18  ReAdmis             10000 non-null  object \n",
      " 19  VitD_levels         10000 non-null  float64\n",
      " 20  Doc_visits          10000 non-null  int64  \n",
      " 21  Full_meals_eaten    10000 non-null  int64  \n",
      " 22  vitD_supp           10000 non-null  int64  \n",
      " 23  Soft_drink          10000 non-null  object \n",
      " 24  Initial_admin       10000 non-null  object \n",
      " 25  HighBlood           10000 non-null  object \n",
      " 26  Stroke              10000 non-null  object \n",
      " 27  Complication_risk   10000 non-null  object \n",
      " 28  Overweight          10000 non-null  object \n",
      " 29  Arthritis           10000 non-null  object \n",
      " 30  Diabetes            10000 non-null  object \n",
      " 31  Hyperlipidemia      10000 non-null  object \n",
      " 32  BackPain            10000 non-null  object \n",
      " 33  Anxiety             10000 non-null  object \n",
      " 34  Allergic_rhinitis   10000 non-null  object \n",
      " 35  Reflux_esophagitis  10000 non-null  object \n",
      " 36  Asthma              10000 non-null  object \n",
      " 37  Services            10000 non-null  object \n",
      " 38  Initial_days        10000 non-null  float64\n",
      " 39  TotalCharge         10000 non-null  float64\n",
      " 40  Additional_charges  10000 non-null  float64\n",
      " 41  Item1               10000 non-null  int64  \n",
      " 42  Item2               10000 non-null  int64  \n",
      " 43  Item3               10000 non-null  int64  \n",
      " 44  Item4               10000 non-null  int64  \n",
      " 45  Item5               10000 non-null  int64  \n",
      " 46  Item6               10000 non-null  int64  \n",
      " 47  Item7               10000 non-null  int64  \n",
      " 48  Item8               10000 non-null  int64  \n",
      "dtypes: float64(7), int64(15), object(27)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, classification_report\n",
    "\n",
    "\n",
    "#load the data ignoring the first column as it's simply an index\n",
    "medical_data = pd.read_csv('./Data/Medical/medical_clean.csv', index_col=0)\n",
    "medical_data.info()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T12:06:47.314136Z",
     "end_time": "2023-04-14T12:06:47.543805Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"data-preparation-goals\"></a>\n",
    "# C1: Data Preparation Goals and Necessary Manipulation\n",
    " * D209 - *Data Mining I* is continuation of datasets from *D206 - Data Cleaning* and is given to us pre-cleaned to a point. Listed below are items pre-completed:\n",
    "     * Detecting Duplicates\n",
    "     * Dealing with missing values\n",
    "     * Detecting Outliers\n",
    " * Hot Encoding (*Dummy Variables* or *Indicator Variables*)\n",
    "     * \"Is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.\" (Vasudev, 2017).\n",
    "     * \"Binary 0-1 variables derived by recoding factor data for use in regression and other models\" (Bruce, P., Bruce, A., & Gedeck, P.  2019 p.163)\n",
    "     * If you have a Series that has 3 categorical levels you can split your series into two variables to represent the three states. If you have blue, yellow, and black and convert them to 1/0's then you only need two columns for the 3 series. For example if you have yellow = 0, and black = 0 then that means your value is blue without having that datapoint. We can apply this to the `Complication Risk` series in the dataset. It includes Low, Medium, and High. If we use the High (Series) and the Low (Series) it can be assumed if High and Low are both 0 values then the value represented is Medium.\n",
    "     * Pandas function for Hot-Encoding is `pd.get_dummies(data)` documentation is [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"variable-selection-and-identification\"></a>\n",
    "# C2: Variable Selection & Identification\n",
    "\n",
    "I am continuing on with the question from D208 so my variables are going to be reused to see if we can get better results with the *k-nearest neighbor (KNN)* model. Below are the variables that were used during logistic regression. Like before survey and census data was not relevant when looking for explanatory variables that were significant to readmission's (`ReAdmin`) to the hospital.\n",
    "\n",
    "| Data Type    | Variable                              |\n",
    "|--------------|---------------------------------------|\n",
    "| Continuous   | \tNumber of Children                   |\n",
    "| Continuous   | \tVitamin D levels                     |\n",
    "| Continuous   | \tNumber of Dr Visits                  |\n",
    "| Continuous   | \tNumber of Vitamin D supplements      |\n",
    "| Continuous   | \tFull meals                           |\n",
    "| Categorical  | Area (Rural, Suburban, Urban)         |\n",
    "| Categorical  | Gender (Male, Female, Nonbinary)      |\n",
    "| Categorical  | Readmission                           |\n",
    "| Categorical  | Soft Drink Consumption                |\n",
    "| Categorical  | Initial Admission Reason              |\n",
    "| Categorical  | High Blood Pressure                   |\n",
    "| Categorical  | Stroke                                |\n",
    "| Categorical  | Complication Risk (Low, Medium, High) |\n",
    "| Categorical  | Overweight                            |\n",
    "| Categorical  | Arthritis                             |\n",
    "| Categorical  | \tDiabetes                             |\n",
    "| Categorical  | \tHyperlipidemia                       |\n",
    "| Categorical  | \tAnxiety                              |\n",
    "| Categorical  | \tAllergic Rhinitis                    |\n",
    "| Categorical\t | Asthma                                |\n",
    "| Categorical\t | Days hospitalized                     |\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"data-preparation\"></a>\n",
    "# C3: Preparation of Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical data does NOT contain any missing values\n",
      "\n",
      "Categoricals with high cardinality to be removed:\n",
      "['Services', 'Timely_Admission', 'Timely_Treatment', 'Timely_Visits', 'Reliability', 'Options', 'Hours_Of_Treatment', 'Courteous_Staff', 'Listening']\n",
      "\n",
      "\n",
      "Categoricals remaining:\n",
      "['Gender', 'ReAdmis', 'Soft_drink', 'Initial_admin', 'HighBlood', 'Stroke', 'Complication_risk', 'Overweight', 'Arthritis', 'Diabetes', 'Hyperlipidemia', 'BackPain', 'Anxiety', 'Allergic_rhinitis', 'Reflux_esophagitis', 'Asthma']\n",
      "\n",
      "\n",
      "Review Each Data Series\n",
      "\n",
      "count    10000.000000\n",
      "mean         2.097200\n",
      "std          2.163659\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          1.000000\n",
      "75%          3.000000\n",
      "max         10.000000\n",
      "Name: Children, dtype: float64\n",
      "\n",
      "count    10000.000000\n",
      "mean        53.511700\n",
      "std         20.638538\n",
      "min         18.000000\n",
      "25%         36.000000\n",
      "50%         53.000000\n",
      "75%         71.000000\n",
      "max         89.000000\n",
      "Name: Age, dtype: float64\n",
      "\n",
      "count    10000.000000\n",
      "mean        17.964262\n",
      "std          2.017231\n",
      "min          9.806483\n",
      "25%         16.626439\n",
      "50%         17.951122\n",
      "75%         19.347963\n",
      "max         26.394449\n",
      "Name: VitD_levels, dtype: float64\n",
      "\n",
      "count    10000.000000\n",
      "mean         0.409000\n",
      "std          0.491674\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          1.000000\n",
      "max          1.000000\n",
      "Name: HighBlood, dtype: float64\n",
      "\n",
      "count    10000.000000\n",
      "mean         0.709400\n",
      "std          0.454062\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          1.000000\n",
      "75%          1.000000\n",
      "max          1.000000\n",
      "Name: Overweight, dtype: float64\n",
      "\n",
      "count    10000.000000\n",
      "mean         0.357400\n",
      "std          0.479258\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          1.000000\n",
      "max          1.000000\n",
      "Name: Arthritis, dtype: float64\n",
      "\n",
      "count    10000.00000\n",
      "mean         0.27380\n",
      "std          0.44593\n",
      "min          0.00000\n",
      "25%          0.00000\n",
      "50%          0.00000\n",
      "75%          1.00000\n",
      "max          1.00000\n",
      "Name: Diabetes, dtype: float64\n",
      "\n",
      "count    10000.000000\n",
      "mean         0.411400\n",
      "std          0.492112\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          1.000000\n",
      "max          1.000000\n",
      "Name: BackPain, dtype: float64\n",
      "\n",
      "count    10000.00000\n",
      "mean         0.28930\n",
      "std          0.45346\n",
      "min          0.00000\n",
      "25%          0.00000\n",
      "50%          0.00000\n",
      "75%          1.00000\n",
      "max          1.00000\n",
      "Name: Asthma, dtype: float64\n",
      "\n",
      "count    10000.000000\n",
      "mean        34.455299\n",
      "std         26.309341\n",
      "min          1.001981\n",
      "25%          7.896215\n",
      "50%         35.836244\n",
      "75%         61.161020\n",
      "max         71.981490\n",
      "Name: Initial_days, dtype: float64\n",
      "\n",
      "count    10000.000000\n",
      "mean         0.366900\n",
      "std          0.481983\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          1.000000\n",
      "max          1.000000\n",
      "Name: ReAdmis, dtype: float64\n",
      "\n",
      "count    10000.000000\n",
      "mean         0.451700\n",
      "std          0.497687\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          1.000000\n",
      "max          1.000000\n",
      "Name: complication_risk_medium, dtype: float64\n",
      "\n",
      "count    10000.000000\n",
      "mean         0.212500\n",
      "std          0.409097\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max          1.000000\n",
      "Name: complication_risk_low, dtype: float64\n",
      "\n",
      "count    10000.000000\n",
      "mean         0.506000\n",
      "std          0.499989\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          1.000000\n",
      "75%          1.000000\n",
      "max          1.000000\n",
      "Name: initial_admission_emergency, dtype: float64\n",
      "\n",
      "count    10000.000000\n",
      "mean         0.243600\n",
      "std          0.429276\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max          1.000000\n",
      "Name: initial_admission_observation, dtype: float64\n",
      "\n",
      "count    10000.000000\n",
      "mean         0.476800\n",
      "std          0.499486\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          1.000000\n",
      "max          1.000000\n",
      "Name: gender_male, dtype: float64\n",
      "\n",
      "count    10000.000000\n",
      "mean         0.021400\n",
      "std          0.144721\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max          1.000000\n",
      "Name: gender_non_binary, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "original_medical = medical_data #storing backup of original data incase it's needed\n",
    "#del medical_clean_data[medical_clean_data.columns[0]]\n",
    "\n",
    "any_missing_values = medical_data.isna().values.any()\n",
    "if not any_missing_values:\n",
    "    print('Medical data does NOT contain any missing values\\n')\n",
    "else:\n",
    "    print('Medical data CONTAINS missing values.\\n')\n",
    "\n",
    "column_renames = {\n",
    "    'Item1': 'Timely_Admission'\n",
    "    ,'Item2': 'Timely_Treatment'\n",
    "    ,'Item3': 'Timely_Visits'\n",
    "    ,'Item4': 'Reliability'\n",
    "    ,'Item5': 'Options'\n",
    "    ,'Item6': 'Hours_Of_Treatment'\n",
    "    ,'Item7': 'Courteous_Staff'\n",
    "    ,'Item8': 'Listening' #Evidence of active listening from Doctor\n",
    "}\n",
    "medical_data.rename(columns=column_renames, inplace=True)\n",
    "#medical_clean_data\n",
    "\n",
    "removal_attributes = ['Customer_id', 'Interaction', 'UID',\n",
    "                      'Zip', 'Lat', 'Lng', 'City', 'State', 'County',\n",
    "                      'Area', 'Job', 'Marital', 'Population', 'TimeZone']\n",
    "\n",
    "medical_clean_data = medical_data.drop(columns=removal_attributes)\n",
    "\n",
    "category_dtype = 'category'\n",
    "convert_to_category = {\n",
    "    'Gender': category_dtype,\n",
    "    'ReAdmis': category_dtype,\n",
    "    'Soft_drink': category_dtype,\n",
    "    'Initial_admin': category_dtype,\n",
    "    'HighBlood': category_dtype,\n",
    "    'Stroke': category_dtype,\n",
    "    'Complication_risk': category_dtype,\n",
    "    'Overweight': category_dtype,\n",
    "    'Arthritis': category_dtype,\n",
    "    'Diabetes': category_dtype,\n",
    "    'Hyperlipidemia': category_dtype,\n",
    "    'BackPain': category_dtype,\n",
    "    'Anxiety': category_dtype,\n",
    "    'Allergic_rhinitis': category_dtype,\n",
    "    'Reflux_esophagitis': category_dtype,\n",
    "    'Asthma': category_dtype,\n",
    "    'Services': category_dtype,\n",
    "    'Timely_Admission': category_dtype,\n",
    "    'Timely_Treatment': category_dtype,\n",
    "    'Timely_Visits': category_dtype,\n",
    "    'Reliability': category_dtype,\n",
    "    'Options': category_dtype,\n",
    "    'Hours_Of_Treatment': category_dtype,\n",
    "    'Courteous_Staff': category_dtype,\n",
    "    'Listening': category_dtype\n",
    "}\n",
    "\n",
    "medical_clean_data = medical_clean_data.astype(convert_to_category)\n",
    "\n",
    "#Logical categorical variables converted to numerical\n",
    "columns_to_reexpress = ['ReAdmis', 'Soft_drink', 'HighBlood', 'Stroke',\n",
    "                        'Overweight', 'Arthritis', 'Diabetes', 'Hyperlipidemia',\n",
    "                        'BackPain', 'Anxiety', 'Allergic_rhinitis', 'Reflux_esophagitis',\n",
    "                        'Asthma']\n",
    "for column in columns_to_reexpress:\n",
    "    medical_clean_data[column] = medical_clean_data[column].map({'Yes': 1, 'No': 0 }).astype(np.int64)\n",
    "\n",
    "categorical_medical_data = medical_clean_data[convert_to_category.keys()]\n",
    "high_cardinalities = categorical_medical_data.nunique() > 3 #(> 3-5 Levels)\n",
    "high_cardinalities = high_cardinalities[high_cardinalities == True]\n",
    "high_cardinalities = list(high_cardinalities.index.values)\n",
    "print('Categoricals with high cardinality to be removed:')\n",
    "print(high_cardinalities)\n",
    "print('\\n')\n",
    "\n",
    "medical_clean_data = medical_clean_data.drop(columns=high_cardinalities)\n",
    "\n",
    "low_cardinalities = [item for item in list(convert_to_category.keys()) if item not in high_cardinalities]\n",
    "print('Categoricals remaining:')\n",
    "print(low_cardinalities)\n",
    "print('\\n')\n",
    "\n",
    "#Re-level ordinal/nominal categoricals\n",
    "complication_risk_dummies = pd.get_dummies(data=medical_clean_data['Complication_risk'], drop_first=True)\n",
    "\n",
    "medical_clean_data['complication_risk_medium'] = complication_risk_dummies['Medium']\n",
    "medical_clean_data['complication_risk_low'] = complication_risk_dummies['Low']\n",
    "\n",
    "initial_admission_dummies = pd.get_dummies(data=medical_clean_data['Initial_admin'], drop_first=True)\n",
    "\n",
    "medical_clean_data['initial_admission_emergency'] = initial_admission_dummies['Emergency Admission']\n",
    "medical_clean_data['initial_admission_observation'] = initial_admission_dummies['Observation Admission']\n",
    "\n",
    "gender_dummies = pd.get_dummies(data=medical_clean_data['Gender'], drop_first=True)\n",
    "\n",
    "medical_clean_data['gender_male'] = gender_dummies['Male']\n",
    "medical_clean_data['gender_non_binary'] = gender_dummies['Nonbinary']\n",
    "\n",
    "regression_variables = ['Children', 'Age', 'VitD_levels', 'HighBlood', 'Overweight', 'Arthritis', 'Diabetes', 'BackPain', 'Asthma', 'Initial_days', 'ReAdmis', 'complication_risk_medium', 'complication_risk_low', 'initial_admission_emergency', 'initial_admission_observation', 'gender_male','gender_non_binary']\n",
    "\n",
    "prepared_medical_data = medical_clean_data[regression_variables]\n",
    "\n",
    "print('Review Each Data Series\\n')\n",
    "for column in prepared_medical_data.columns:\n",
    "    current = prepared_medical_data[column]\n",
    "    if np.issubdtype(current.dtype, np.number):\n",
    "        print(f'{current.describe()}\\n')\n",
    "    else:\n",
    "        print(f'{current.value_counts()}\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T12:06:47.415523Z",
     "end_time": "2023-04-14T12:06:47.564357Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"copy-of-prepared-data\"></a>\n",
    "# Copy of Prepared Data Set\n",
    "\n",
    "##### Preparation Lirbary Assistance:\n",
    " * To aid the in the variable selection process we will be using [`SelectKBest`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)\n",
    " * To check for Multicollinearity we will use the library [`variance_inflation_factor`](https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3 entries, 9 to 12\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Feature   3 non-null      object \n",
      " 1   p_values  3 non-null      float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 72.0+ bytes\n",
      "None\n",
      "                        Feature  p_values\n",
      "9                  Initial_days  0.000000\n",
      "0                      Children  0.018613\n",
      "12  initial_admission_emergency  0.048766\n",
      "['Initial_days', 'Children', 'initial_admission_emergency']\n"
     ]
    }
   ],
   "source": [
    "copy_of_prepared_medical_data = prepared_medical_data.copy()\n",
    "copy_of_prepared_medical_data.to_csv('./prepared-medical-data-for-feature-selection.csv')\n",
    "\n",
    "dependent_variable = prepared_medical_data['ReAdmis']\n",
    "explanatory_variables = prepared_medical_data.drop(columns=['ReAdmis'])\n",
    "\n",
    "#print(dependent_variable.info())\n",
    "#print(explanatory_variables.info())\n",
    "\n",
    "#We'll take our explanatory variables and run SelectKBest function on them to select the best features for the model based against the dependent variable\n",
    "feature_selection = SelectKBest(f_classif, k='all')\n",
    "feature_selection.fit(explanatory_variables, dependent_variable)\n",
    "\n",
    "feature_column = 'Feature'\n",
    "p_value_column = 'p_values'\n",
    "\n",
    "f_pvalues = { feature_column: explanatory_variables.columns, p_value_column: feature_selection.pvalues_ }\n",
    "feature_selection_pvalues = pd.DataFrame(f_pvalues)\n",
    "feature_selection_pvalues = feature_selection_pvalues.sort_values(p_value_column)\n",
    "\n",
    "kept_features = feature_selection_pvalues[feature_selection_pvalues[p_value_column] < .05]\n",
    "\n",
    "print(kept_features.info())\n",
    "print(kept_features)\n",
    "\n",
    "kept_features.to_csv('./feature-selected-medical-data.csv')\n",
    "\n",
    "selected_features_columns = list(kept_features[feature_column])\n",
    "print(selected_features_columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T12:06:47.518564Z",
     "end_time": "2023-04-14T12:06:47.637552Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"data-splitting-and-copying\"></a>\n",
    "# D1: Data Splitting, Copy of Split Data\n",
    "\n",
    "Get Training and Test data for `features` and `dependent variable` using train_test_split(...) function.\n",
    "\n",
    "To make sure that the linear regression aspect using the correct value for intercept we will use the [`add_constant()`](https://www.statsmodels.org/stable/generated/statsmodels.tools.tools.add_constant.html)\n",
    "\n",
    "Training and Test data will be split into 80% training and 20% test data for explanatory variables and dependent variable\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "X_explanatory = sm.add_constant(explanatory_variables[selected_features_columns])\n",
    "y_target = dependent_variable\n",
    "\n",
    "print(X_explanatory.shape)\n",
    "print(y_target.shape)\n",
    "\n",
    "X_explanatory_training, X_explanatory_test, y_target_training, y_target_testing = train_test_split(X_explanatory, y_target, test_size=.2, random_state=42, stratify=y_target)\n",
    "\n",
    "#Save training and test datas\n",
    "X_explanatory_training.to_csv('./x_explanatory-training-dataset.csv')\n",
    "X_explanatory_test.to_csv('./x_explanatory-testing-dataset.csv')\n",
    "\n",
    "y_target_training.to_csv('./y_target-training-dataset.csv')\n",
    "y_target_testing.to_csv('./y_target-testing-dataset.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T12:06:47.594426Z",
     "end_time": "2023-04-14T12:06:47.684981Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"analysis-description\"></a>\n",
    "# D2: Analysis Description"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Oh noes n' stuff!",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [85]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOh noes n\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m stuff!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mException\u001B[0m: Oh noes n' stuff!"
     ]
    }
   ],
   "source": [
    "raise Exception(\"Oh noes n' stuff!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T19:45:16.356680Z",
     "end_time": "2023-04-03T19:45:16.395334Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"classification-analysis-code\"></a>\n",
    "# D3: Classification Analysis Code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raise Exception(\"Oh noes n' stuff!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T19:45:16.372255Z",
     "end_time": "2023-04-03T19:45:16.432064Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"accuracy-of-classification-model\"></a>\n",
    "# E1: Accuracy of Classification Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raise Exception(\"Oh noes n' stuff!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T19:45:16.387313Z",
     "end_time": "2023-04-03T19:45:16.432064Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"model-results\"></a>\n",
    "# E2: Model Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raise Exception(\"Oh noes n' stuff!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T19:45:16.404846Z",
     "end_time": "2023-04-03T19:45:16.432064Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"classification-limitations\"></a>\n",
    "# E3: Classification Limitations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raise Exception(\"Oh noes n' stuff!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T19:45:16.418893Z",
     "end_time": "2023-04-03T19:45:16.439063Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"recommended-action\"></a>\n",
    "# E4: Recommended Action"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raise Exception(\"Oh noes n' stuff!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T19:45:16.435064Z",
     "end_time": "2023-04-03T19:45:16.449120Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"panopto-recording\"></a>\n",
    "# F: Panopto Recording"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raise Exception(\"Oh noes n' stuff!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T19:45:16.449120Z",
     "end_time": "2023-04-03T19:45:16.487242Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"code-references\"></a>\n",
    "# G: Code References\n",
    "\n",
    "No References Used."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"source-references\"></a>\n",
    "# H: Source References\n",
    "\n",
    "#### Citations\n",
    " * Bruce, P., Bruce, A., & Gedeck, P. (2019). Practical Statistics for Data Scientists: 50 Essential Concepts. O'Reilly Media, Inc.\n",
    " * Tavva, R. (2020, October 10). K Nearest Neighbour For Supervised Learning. Data Science Blog. [https://data-science-blog.com/blog/2020/10/10/k-nearest-neighbour-for-supervised-learning/](https://data-science-blog.com/blog/2020/10/10/k-nearest-neighbour-for-supervised-learning/)\n",
    " * Cover, T., & Hart, P. (1967). Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13(1), 21-27. Retrieved from [https://ieeexplore.ieee.org/document/1054010](https://ieeexplore.ieee.org/document/1054010)\n",
    " * Dasarathy, B. V. (1991). Nearest neighbor (NN) norms: NN pattern classification techniques. IEEE Computer Society Press. Retrieved from [https://ieeexplore.ieee.org/document/134048](https://ieeexplore.ieee.org/document/134048)\n",
    " * Aggarwal, C. C., Hinneburg, A., & Keim, D. A. (2001). On the surprising behavior of distance metrics in high dimensional space. International Conference on Database Theory, 420-434. Retrieved from [https://link.springer.com/chapter/10.1007/3-540-44503-X_25](https://link.springer.com/chapter/10.1007/3-540-44503-X_25)\n",
    " * Citation: Batista, G. E., Prati, R. C., & Monard, M. C. (2004). A study of the behavior of several methods for balancing machine learning training data. ACM SIGKDD Explorations Newsletter, 6(1), 20-29. Retrieved from [https://dl.acm.org/doi/10.1145/1007730.1007735](https://dl.acm.org/doi/10.1145/1007730.1007735)\n",
    " * Wang, Y., & Wong, K. C. (2008). Enhancing K-nearest neighbor classifier with local information. International Journal of Pattern Recognition and Artificial Intelligence, 22(01), 135-154. Retrieved from [https://www.worldscientific.com/doi/abs/10.1142/S0218001408006325](https://www.worldscientific.com/doi/abs/10.1142/S0218001408006325)\n",
    "  * Vasudev. (2017). What is One Hot Encoding? Why and When Do You Have to Use it? HackerNoon. [https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
