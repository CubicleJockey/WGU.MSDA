{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# D209 Data Mining I Performance Assessment - Task 1\n",
    "### NVM2 — NVM2 Task 1: Classification Analysis\n",
    "#### Data Mining I — D209\n",
    "#### PRFA — NVM2\n",
    "> André Davis\n",
    "> StudentID: 010630641\n",
    "> MSDA\n",
    ">\n",
    "> Competencies\n",
    "> 4030.06.1 : Classification Data Mining Models\n",
    ">   The graduate applies observations to appropriate classes and categories using classification models.\n",
    ">\n",
    "> 4030.06.3 : Data Mining Model Performance\n",
    ">   The graduate evaluates data mining model performance for precision, accuracy, and model comparison.\n",
    "\n",
    "#### Table of Contents\n",
    "<ul>\n",
    "    <li><a href=\"#research-question\">A1: Research Question</li>\n",
    "    <li><a href=\"#objectives-and-goals\">A2: Goals of Analysis</a></li>\n",
    "    <li><a href=\"#justification\">B1: Justification of Classification Method</a></li>\n",
    "    <li><a href=\"#classification-model\">B2: Assumption of a Classification Model</a></li>\n",
    "    <li><a href=\"#packages-and-analysis-support\">B3: List Packages & Support of Analysis</a></li>\n",
    "    <li><a href=\"#data-preparation-goals\">C1: Data Preparation Goals and Necessary Manipulation</a></li>\n",
    "    <li><a href=\"#variable-selection-and-identification\">C2: Variable Selection \\& Identification</a></li>\n",
    "    <li><a href=\"#data-preparation\">C3: Preparation of Data</a></li>\n",
    "    <li><a href=\"#copy-of-prepared-data\">C4: Copy of Prepared Data Set</a></li>\n",
    "    <li><a href=\"#data-splitting-and-copying\">D1: Data Splitting, Copy of Split Data</a></li>\n",
    "    <li><a href=\"#analysis-description\">D2: Analysis Description</a></li>\n",
    "    <li><a href=\"#classification-analysis-code\">D3: Classification Analysis Code</a></li>\n",
    "    <li><a href=\"#accuracy-of-classification-model\">E1: Accuracy of Classification Model</a></li>\n",
    "    <li><a href=\"#model-results\">E2: Model Results</a></li>\n",
    "    <li><a href=\"#classification-limitations\">E3: Classification Limitations</a></li>\n",
    "    <li><a href=\"#recommended-action\">E4: Recommended Action</a></li>\n",
    "    <li><a href=\"#panopto-recording\">F: Panopto Recording</a></li>\n",
    "    <li><a href=\"#code-references\">G: Code References</a></li>\n",
    "    <li><a href=\"#source-references\">H: Source References</a></li>\n",
    "</ul>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"research-question\"></a>\n",
    "# A1: Research Question\n",
    "\n",
    "We will continue down the path that was set forth in the second task of D208, which used logical regression. The research question that will be focused of D209 Task 1 - Classification Analysis is `\"Which factors have a significant effect on readmission?\"`. The main goal of the classification analysis is to see if a more effective model can be created and give better details than D209 logistic regression model. The available options for this task are ['k-nearest neighbor (KNN)'](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) and ['Naive Bayes'](https://en.wikipedia.org/wiki/Naive_Bayes_classifier). I will be choosing *k-nearest neighbor (KNN)* to perform my classification analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"objectives-and-goals\"></a>\n",
    "# A2: Objective and Goals of Analysis\n",
    "\n",
    "At the end of Task 2 of D208 Logistical Regression a few explanatory variables presented themselves as things a hospital could look at or address in regard to readmission to the hospital. Even though the model did find some items for the hospital to work on we are going to use *k-nearest neighbor (KNN)* to see if the question can be answered more detailed via this model. The goal is attempting to create a better model for the question of readmitting patients is beneficial to the patient, hospital, and the laws in place to keep hospital readmission low. The hospital's core responsibility lies in handling existing issues and averting possible future ones. By proactively and efficiently managing these concerns, the hospital can improve outcomes for its present patients, avoid extending their hospital stay, prevent readmission for the same issue, and even avert potential complications or health risks that could lead to future hospitalization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"justification\"></a>\n",
    "# B1: Justification of Classification Method\n",
    "\n",
    "##### k-nearest neighbor (KNN)\n",
    "---\n",
    "\n",
    "The chosen method is *k-nearest neighbor (KNN)*. \"KNN is one of the simpler prediction/classification techniques: there is no model to be fit (as in regression). This doesn't mean that using KNN is an automatic procedure. The prediction results depend on how the features are scaled, how similarity is measured, and how big *K* is set. Also, all predictors must be in numeric form.\" (Bruce, Bruce, & Gedeck, 2019).\n",
    "\n",
    "KNN is a non-parametric algorithm that does not make explicit assumptions about the underlying distribution of the data. In KNN, the prediction for a new data point is based on the classification or value of regression of its nearest neighbors in the training data. The value of *K* represents the number of nearest neighbors considered. *K* will remain constant as new datapoints are being introduced.\n",
    "\n",
    "This method seems to be useful when working with the hospital data and questions as it allows new data points to be introduced and get classified to its proper section based on the surrounding data it lands by. This seems really flexible and a great way to classify out data appropriately.\n",
    "\n",
    "The biggest advantages of *KNN* include:\n",
    " - Quickest Calculation Time *(Tavva, 2020)*\n",
    " - Simple Algorithms *(Tavva, 2020)*\n",
    " - High Accuracy *(Tavva, 2020)*\n",
    " - Versatile – best use for Regression and Classification. *(Tavva, 2020)*\n",
    " - Doesn’t make any assumptions about data. *(Tavva, 2020)*\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"classification-model\"></a>\n",
    "# B2: Assumptions of a Classification Model\n",
    "\n",
    "##### K-Nearest Neighbor (KNN) Assumptions:\n",
    "\n",
    " * Homogeneous feature space:\n",
    "    - KNN assumes that the feature space is homogeneous, meaning that all features have the same scale and importance. If the features are not homogeneous, some features may be given more weight than others, leading to biased results. (Cover and Hart (1967))\n",
    "\n",
    " * The relevance of the distance metric:\n",
    "     * KNN relies heavily on the choice of distance metric used to measure similarity between instances. The algorithm assumes that the distance metric used is appropriate for the dataset and the problem being solved. Therefore, choosing an appropriate distance metric is critical to the performance of KNN. (Dasarathy (1991))\n",
    "\n",
    " * High dimensionality:\n",
    "     * KNN is sensitive to the curse of dimensionality, where the number of features is significantly higher than the number of instances. In high-dimensional spaces, the distance between the nearest neighbors becomes meaningless, making KNN less effective. (Aggarwal et al. (2001))\n",
    "\n",
    " * Imbalanced data:\n",
    "      * KNN assumes that the classes are equally distributed in the dataset. If the dataset is imbalanced, the algorithm may favor the majority class, leading to inaccurate results. (Batista et al. (2004))\n",
    "\n",
    " * Noisy data:\n",
    "     * KNN is susceptible to noisy data, which may introduce errors in the classification or regression process. To mitigate the effect of noisy data, preprocessing techniques such as outlier removal or noise reduction may be applied. (Wang and Wong (2008))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"packages-and-analysis-support\"></a>\n",
    "# B3: List Packages & Support of Analysis\n",
    "\n",
    "#### Analytic package for Python:\n",
    " * **Data manipulation:**\n",
    "    * [Pandas](https://pandas.pydata.org/docs/)\n",
    "      - Pandas library is used for data manipulation, analysis, and cleaning. Pandas' main data structures are the Series (1-dimensional) and DataFrame (2-dimensional).\n",
    "          - [Version 2](https://towardsdatascience.com/whats-new-in-pandas-2-0-5df366eb0197) is coming soon and will replace NumPy with Apache Arrows.\n",
    "    * [NumPy](https://numpy.org/doc/)\n",
    "         - Library used for numerical computing. It provides support for multidimensional arrays, matrices, and high-level mathematical functions to perform complex computations quickly and efficiently.\n",
    " * **Statistics:**\n",
    "    * [VIF - Variance Inflation Factor](https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html)\n",
    "        * *variance_inflation_factor* calculates the variance inflation factor (VIF) for a set of predictor variables in a linear regression model, which is a measure of the degree of multicollinearity between the predictors.\n",
    " * **Displaying Data:**\n",
    "    * [Seaborn](https://seaborn.pydata.org/)\n",
    "        * This is a data visualization library.\n",
    "    * [MatPlotLib](https://matplotlib.org/)\n",
    "        * This is a data visualization library.\n",
    " * **Analysis:**\n",
    "    * SkLearn (Scikit-learn - Machine Learning):\n",
    "        * [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "        * [preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "        * [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)\n",
    "        * [f_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html)\n",
    "        * [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "        * [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "        * [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "        * [roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)\n",
    "        * [roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)\n",
    "        * [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10000 entries, 1 to 10000\n",
      "Data columns (total 49 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Customer_id         10000 non-null  object \n",
      " 1   Interaction         10000 non-null  object \n",
      " 2   UID                 10000 non-null  object \n",
      " 3   City                10000 non-null  object \n",
      " 4   State               10000 non-null  object \n",
      " 5   County              10000 non-null  object \n",
      " 6   Zip                 10000 non-null  int64  \n",
      " 7   Lat                 10000 non-null  float64\n",
      " 8   Lng                 10000 non-null  float64\n",
      " 9   Population          10000 non-null  int64  \n",
      " 10  Area                10000 non-null  object \n",
      " 11  TimeZone            10000 non-null  object \n",
      " 12  Job                 10000 non-null  object \n",
      " 13  Children            10000 non-null  int64  \n",
      " 14  Age                 10000 non-null  int64  \n",
      " 15  Income              10000 non-null  float64\n",
      " 16  Marital             10000 non-null  object \n",
      " 17  Gender              10000 non-null  object \n",
      " 18  ReAdmis             10000 non-null  object \n",
      " 19  VitD_levels         10000 non-null  float64\n",
      " 20  Doc_visits          10000 non-null  int64  \n",
      " 21  Full_meals_eaten    10000 non-null  int64  \n",
      " 22  vitD_supp           10000 non-null  int64  \n",
      " 23  Soft_drink          10000 non-null  object \n",
      " 24  Initial_admin       10000 non-null  object \n",
      " 25  HighBlood           10000 non-null  object \n",
      " 26  Stroke              10000 non-null  object \n",
      " 27  Complication_risk   10000 non-null  object \n",
      " 28  Overweight          10000 non-null  object \n",
      " 29  Arthritis           10000 non-null  object \n",
      " 30  Diabetes            10000 non-null  object \n",
      " 31  Hyperlipidemia      10000 non-null  object \n",
      " 32  BackPain            10000 non-null  object \n",
      " 33  Anxiety             10000 non-null  object \n",
      " 34  Allergic_rhinitis   10000 non-null  object \n",
      " 35  Reflux_esophagitis  10000 non-null  object \n",
      " 36  Asthma              10000 non-null  object \n",
      " 37  Services            10000 non-null  object \n",
      " 38  Initial_days        10000 non-null  float64\n",
      " 39  TotalCharge         10000 non-null  float64\n",
      " 40  Additional_charges  10000 non-null  float64\n",
      " 41  Item1               10000 non-null  int64  \n",
      " 42  Item2               10000 non-null  int64  \n",
      " 43  Item3               10000 non-null  int64  \n",
      " 44  Item4               10000 non-null  int64  \n",
      " 45  Item5               10000 non-null  int64  \n",
      " 46  Item6               10000 non-null  int64  \n",
      " 47  Item7               10000 non-null  int64  \n",
      " 48  Item8               10000 non-null  int64  \n",
      "dtypes: float64(7), int64(15), object(27)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, classification_report\n",
    "\n",
    "\n",
    "\n",
    "#load the data ignoring the first column as it's simply an index\n",
    "medical_data = pd.read_csv('./medical_clean.csv', index_col=0)\n",
    "medical_data.info()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T17:59:14.548581900Z",
     "start_time": "2023-11-22T17:59:13.288531500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"data-preparation-goals\"></a>\n",
    "# C1: Data Preparation Goals and Necessary Manipulation\n",
    " * D209 - *Data Mining I* is continuation of datasets from *D206 - Data Cleaning* and is given to us pre-cleaned to a point. Listed below are items pre-completed:\n",
    "     * Detecting Duplicates\n",
    "     * Dealing with missing values\n",
    "     * Detecting Outliers\n",
    " * Hot Encoding (*Dummy Variables* or *Indicator Variables*)\n",
    "     * \"Is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.\" (Vasudev, 2017).\n",
    "     * \"Binary 0-1 variables derived by recoding factor data for use in regression and other models\" (Bruce, P., Bruce, A., & Gedeck, P.  2019 p.163)\n",
    "     * If you have a Series that has 3 categorical levels you can split your series into two variables to represent the three states. If you have blue, yellow, and black and convert them to 1/0's then you only need two columns for the 3 series. For example if you have yellow = 0, and black = 0 then that means your value is blue without having that datapoint. We can apply this to the `Complication Risk` series in the dataset. It includes Low, Medium, and High. If we use the High (Series) and the Low (Series) it can be assumed if High and Low are both 0 values then the value represented is Medium.\n",
    "     * Pandas function for Hot-Encoding is `pd.get_dummies(data)` documentation is [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"variable-selection-and-identification\"></a>\n",
    "# C2: Variable Selection & Identification\n",
    "\n",
    "I am continuing on with the question from D208 so my variables are going to be reused to see if we can get better results with the *k-nearest neighbor (KNN)* model. Below are the variables that were used during logistic regression. Like before survey and census data was not relevant when looking for explanatory variables that were significant to readmission's (`ReAdmin`) to the hospital.\n",
    "\n",
    "| Data Type    | Variable                              |\n",
    "|--------------|---------------------------------------|\n",
    "| Continuous   | \tNumber of Children                   |\n",
    "| Continuous   | \tVitamin D levels                     |\n",
    "| Continuous   | \tNumber of Dr Visits                  |\n",
    "| Continuous   | \tNumber of Vitamin D supplements      |\n",
    "| Continuous   | \tFull meals                           |\n",
    "| Categorical  | Area (Rural, Suburban, Urban)         |\n",
    "| Categorical  | Gender (Male, Female, Nonbinary)      |\n",
    "| Categorical  | Readmission                           |\n",
    "| Categorical  | Soft Drink Consumption                |\n",
    "| Categorical  | Initial Admission Reason              |\n",
    "| Categorical  | High Blood Pressure                   |\n",
    "| Categorical  | Stroke                                |\n",
    "| Categorical  | Complication Risk (Low, Medium, High) |\n",
    "| Categorical  | Overweight                            |\n",
    "| Categorical  | Arthritis                             |\n",
    "| Categorical  | \tDiabetes                             |\n",
    "| Categorical  | \tHyperlipidemia                       |\n",
    "| Categorical  | \tAnxiety                              |\n",
    "| Categorical  | \tAllergic Rhinitis                    |\n",
    "| Categorical\t | Asthma                                |\n",
    "| Categorical\t | Days hospitalized                     |\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"data-preparation\"></a>\n",
    "# C3: Preparation of Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical data does NOT contain any missing values\n",
      "\n",
      "Categoricals with high cardinality to be removed:\n",
      "['Services', 'Timely_Admission', 'Timely_Treatment', 'Timely_Visits', 'Reliability', 'Options', 'Hours_Of_Treatment', 'Courteous_Staff', 'Listening']\n",
      "\n",
      "\n",
      "Categoricals remaining:\n",
      "['Gender', 'ReAdmis', 'Soft_drink', 'Initial_admin', 'HighBlood', 'Stroke', 'Complication_risk', 'Overweight', 'Arthritis', 'Diabetes', 'Hyperlipidemia', 'BackPain', 'Anxiety', 'Allergic_rhinitis', 'Reflux_esophagitis', 'Asthma']\n",
      "\n",
      "\n",
      "Review Each Data Series\n",
      "\n",
      "count    10000.000000\n",
      "mean         2.097200\n",
      "std          2.163659\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          1.000000\n",
      "75%          3.000000\n",
      "max         10.000000\n",
      "Name: Children, dtype: float64\n",
      "\n",
      "count    10000.000000\n",
      "mean        53.511700\n",
      "std         20.638538\n",
      "min         18.000000\n",
      "25%         36.000000\n",
      "50%         53.000000\n",
      "75%         71.000000\n",
      "max         89.000000\n",
      "Name: Age, dtype: float64\n",
      "\n",
      "count    10000.000000\n",
      "mean        17.964262\n",
      "std          2.017231\n",
      "min          9.806483\n",
      "25%         16.626439\n",
      "50%         17.951122\n",
      "75%         19.347963\n",
      "max         26.394449\n",
      "Name: VitD_levels, dtype: float64\n",
      "\n",
      "count    10000.000000\n",
      "mean         0.409000\n",
      "std          0.491674\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          1.000000\n",
      "max          1.000000\n",
      "Name: HighBlood, dtype: float64\n",
      "\n",
      "count    10000.000000\n",
      "mean         0.709400\n",
      "std          0.454062\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          1.000000\n",
      "75%          1.000000\n",
      "max          1.000000\n",
      "Name: Overweight, dtype: float64\n",
      "\n",
      "count    10000.000000\n",
      "mean         0.357400\n",
      "std          0.479258\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          1.000000\n",
      "max          1.000000\n",
      "Name: Arthritis, dtype: float64\n",
      "\n",
      "count    10000.00000\n",
      "mean         0.27380\n",
      "std          0.44593\n",
      "min          0.00000\n",
      "25%          0.00000\n",
      "50%          0.00000\n",
      "75%          1.00000\n",
      "max          1.00000\n",
      "Name: Diabetes, dtype: float64\n",
      "\n",
      "count    10000.000000\n",
      "mean         0.411400\n",
      "std          0.492112\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          1.000000\n",
      "max          1.000000\n",
      "Name: BackPain, dtype: float64\n",
      "\n",
      "count    10000.00000\n",
      "mean         0.28930\n",
      "std          0.45346\n",
      "min          0.00000\n",
      "25%          0.00000\n",
      "50%          0.00000\n",
      "75%          1.00000\n",
      "max          1.00000\n",
      "Name: Asthma, dtype: float64\n",
      "\n",
      "count    10000.000000\n",
      "mean        34.455299\n",
      "std         26.309341\n",
      "min          1.001981\n",
      "25%          7.896215\n",
      "50%         35.836244\n",
      "75%         61.161020\n",
      "max         71.981490\n",
      "Name: Initial_days, dtype: float64\n",
      "\n",
      "count    10000.000000\n",
      "mean         0.366900\n",
      "std          0.481983\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          1.000000\n",
      "max          1.000000\n",
      "Name: ReAdmis, dtype: float64\n",
      "\n",
      "complication_risk_medium\n",
      "False    5483\n",
      "True     4517\n",
      "Name: count, dtype: int64\n",
      "\n",
      "complication_risk_low\n",
      "False    7875\n",
      "True     2125\n",
      "Name: count, dtype: int64\n",
      "\n",
      "initial_admission_emergency\n",
      "True     5060\n",
      "False    4940\n",
      "Name: count, dtype: int64\n",
      "\n",
      "initial_admission_observation\n",
      "False    7564\n",
      "True     2436\n",
      "Name: count, dtype: int64\n",
      "\n",
      "gender_male\n",
      "False    5232\n",
      "True     4768\n",
      "Name: count, dtype: int64\n",
      "\n",
      "gender_non_binary\n",
      "False    9786\n",
      "True      214\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "original_medical = medical_data #storing backup of original data incase it's needed\n",
    "#del medical_clean_data[medical_clean_data.columns[0]]\n",
    "\n",
    "any_missing_values = medical_data.isna().values.any()\n",
    "if not any_missing_values:\n",
    "    print('Medical data does NOT contain any missing values\\n')\n",
    "else:\n",
    "    print('Medical data CONTAINS missing values.\\n')\n",
    "\n",
    "column_renames = {\n",
    "    'Item1': 'Timely_Admission'\n",
    "    ,'Item2': 'Timely_Treatment'\n",
    "    ,'Item3': 'Timely_Visits'\n",
    "    ,'Item4': 'Reliability'\n",
    "    ,'Item5': 'Options'\n",
    "    ,'Item6': 'Hours_Of_Treatment'\n",
    "    ,'Item7': 'Courteous_Staff'\n",
    "    ,'Item8': 'Listening' #Evidence of active listening from Doctor\n",
    "}\n",
    "medical_data.rename(columns=column_renames, inplace=True)\n",
    "#medical_clean_data\n",
    "\n",
    "removal_attributes = ['Customer_id', 'Interaction', 'UID',\n",
    "                      'Zip', 'Lat', 'Lng', 'City', 'State', 'County',\n",
    "                      'Area', 'Job', 'Marital', 'Population', 'TimeZone']\n",
    "\n",
    "medical_clean_data = medical_data.drop(columns=removal_attributes)\n",
    "\n",
    "category_dtype = 'category'\n",
    "convert_to_category = {\n",
    "    'Gender': category_dtype,\n",
    "    'ReAdmis': category_dtype,\n",
    "    'Soft_drink': category_dtype,\n",
    "    'Initial_admin': category_dtype,\n",
    "    'HighBlood': category_dtype,\n",
    "    'Stroke': category_dtype,\n",
    "    'Complication_risk': category_dtype,\n",
    "    'Overweight': category_dtype,\n",
    "    'Arthritis': category_dtype,\n",
    "    'Diabetes': category_dtype,\n",
    "    'Hyperlipidemia': category_dtype,\n",
    "    'BackPain': category_dtype,\n",
    "    'Anxiety': category_dtype,\n",
    "    'Allergic_rhinitis': category_dtype,\n",
    "    'Reflux_esophagitis': category_dtype,\n",
    "    'Asthma': category_dtype,\n",
    "    'Services': category_dtype,\n",
    "    'Timely_Admission': category_dtype,\n",
    "    'Timely_Treatment': category_dtype,\n",
    "    'Timely_Visits': category_dtype,\n",
    "    'Reliability': category_dtype,\n",
    "    'Options': category_dtype,\n",
    "    'Hours_Of_Treatment': category_dtype,\n",
    "    'Courteous_Staff': category_dtype,\n",
    "    'Listening': category_dtype\n",
    "}\n",
    "\n",
    "medical_clean_data = medical_clean_data.astype(convert_to_category)\n",
    "\n",
    "#Logical categorical variables converted to numerical\n",
    "columns_to_reexpress = ['ReAdmis', 'Soft_drink', 'HighBlood', 'Stroke',\n",
    "                        'Overweight', 'Arthritis', 'Diabetes', 'Hyperlipidemia',\n",
    "                        'BackPain', 'Anxiety', 'Allergic_rhinitis', 'Reflux_esophagitis',\n",
    "                        'Asthma']\n",
    "for column in columns_to_reexpress:\n",
    "    medical_clean_data[column] = medical_clean_data[column].map({'Yes': 1, 'No': 0 }).astype(np.int64)\n",
    "\n",
    "categorical_medical_data = medical_clean_data[convert_to_category.keys()]\n",
    "high_cardinalities = categorical_medical_data.nunique() > 3 #(> 3-5 Levels)\n",
    "high_cardinalities = high_cardinalities[high_cardinalities == True]\n",
    "high_cardinalities = list(high_cardinalities.index.values)\n",
    "print('Categoricals with high cardinality to be removed:')\n",
    "print(high_cardinalities)\n",
    "print('\\n')\n",
    "\n",
    "medical_clean_data = medical_clean_data.drop(columns=high_cardinalities)\n",
    "\n",
    "low_cardinalities = [item for item in list(convert_to_category.keys()) if item not in high_cardinalities]\n",
    "print('Categoricals remaining:')\n",
    "print(low_cardinalities)\n",
    "print('\\n')\n",
    "\n",
    "#Re-level ordinal/nominal categoricals\n",
    "complication_risk_dummies = pd.get_dummies(data=medical_clean_data['Complication_risk'], drop_first=True)\n",
    "\n",
    "medical_clean_data['complication_risk_medium'] = complication_risk_dummies['Medium']\n",
    "medical_clean_data['complication_risk_low'] = complication_risk_dummies['Low']\n",
    "\n",
    "initial_admission_dummies = pd.get_dummies(data=medical_clean_data['Initial_admin'], drop_first=True)\n",
    "\n",
    "medical_clean_data['initial_admission_emergency'] = initial_admission_dummies['Emergency Admission']\n",
    "medical_clean_data['initial_admission_observation'] = initial_admission_dummies['Observation Admission']\n",
    "\n",
    "gender_dummies = pd.get_dummies(data=medical_clean_data['Gender'], drop_first=True)\n",
    "\n",
    "medical_clean_data['gender_male'] = gender_dummies['Male']\n",
    "medical_clean_data['gender_non_binary'] = gender_dummies['Nonbinary']\n",
    "\n",
    "regression_variables = ['Children', 'Age', 'VitD_levels', 'HighBlood', 'Overweight', 'Arthritis', 'Diabetes', 'BackPain', 'Asthma', 'Initial_days', 'ReAdmis', 'complication_risk_medium', 'complication_risk_low', 'initial_admission_emergency', 'initial_admission_observation', 'gender_male','gender_non_binary']\n",
    "\n",
    "prepared_medical_data = medical_clean_data[regression_variables]\n",
    "\n",
    "print('Review Each Data Series\\n')\n",
    "for column in prepared_medical_data.columns:\n",
    "    current = prepared_medical_data[column]\n",
    "    if np.issubdtype(current.dtype, np.number):\n",
    "        print(f'{current.describe()}\\n')\n",
    "    else:\n",
    "        print(f'{current.value_counts()}\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T17:59:14.601982500Z",
     "start_time": "2023-11-22T17:59:14.552571800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"copy-of-prepared-data\"></a>\n",
    "# C4: Copy of Prepared Data Set\n",
    "\n",
    "##### Preparation Libraries Assistance:\n",
    " * To aid the in the variable selection process we will be using [`SelectKBest`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)\n",
    " * To check for Multicollinearity we will use the library [`variance_inflation_factor`](https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3 entries, 9 to 12\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Feature   3 non-null      object \n",
      " 1   p_values  3 non-null      float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 72.0+ bytes\n",
      "None\n",
      "                        Feature  p_values\n",
      "9                  Initial_days  0.000000\n",
      "0                      Children  0.018613\n",
      "12  initial_admission_emergency  0.048766\n",
      "['Initial_days', 'Children', 'initial_admission_emergency']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 40\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# Of the selected features, we are going to check them for `multicollinearity`\u001B[39;00m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;66;03m# We don't want any highly correlated dependent variables as this would cause redundency within the model.\u001B[39;00m\n\u001B[0;32m     35\u001B[0m multicollinearity_features_to_check \u001B[38;5;241m=\u001B[39m prepared_medical_data[selected_features_columns]\n\u001B[0;32m     38\u001B[0m vif_data \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeature\u001B[39m\u001B[38;5;124m'\u001B[39m: multicollinearity_features_to_check\u001B[38;5;241m.\u001B[39mcolumns,\n\u001B[1;32m---> 40\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mVIF\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[43m[\u001B[49m\u001B[43mvariance_inflation_factor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmulticollinearity_features_to_check\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmulticollinearity_features_to_check\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     41\u001B[0m }\n\u001B[0;32m     42\u001B[0m vif_features \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(vif_data)\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28mprint\u001B[39m(vif_features)\n",
      "Cell \u001B[1;32mIn[3], line 40\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# Of the selected features, we are going to check them for `multicollinearity`\u001B[39;00m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;66;03m# We don't want any highly correlated dependent variables as this would cause redundency within the model.\u001B[39;00m\n\u001B[0;32m     35\u001B[0m multicollinearity_features_to_check \u001B[38;5;241m=\u001B[39m prepared_medical_data[selected_features_columns]\n\u001B[0;32m     38\u001B[0m vif_data \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeature\u001B[39m\u001B[38;5;124m'\u001B[39m: multicollinearity_features_to_check\u001B[38;5;241m.\u001B[39mcolumns,\n\u001B[1;32m---> 40\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mVIF\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[43mvariance_inflation_factor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmulticollinearity_features_to_check\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(multicollinearity_features_to_check\u001B[38;5;241m.\u001B[39mcolumns))]\n\u001B[0;32m     41\u001B[0m }\n\u001B[0;32m     42\u001B[0m vif_features \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(vif_data)\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28mprint\u001B[39m(vif_features)\n",
      "File \u001B[1;32mC:\\GitHub\\WGU.MSDA\\venv\\Lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:197\u001B[0m, in \u001B[0;36mvariance_inflation_factor\u001B[1;34m(exog, exog_idx)\u001B[0m\n\u001B[0;32m    195\u001B[0m mask \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marange(k_vars) \u001B[38;5;241m!=\u001B[39m exog_idx\n\u001B[0;32m    196\u001B[0m x_noti \u001B[38;5;241m=\u001B[39m exog[:, mask]\n\u001B[1;32m--> 197\u001B[0m r_squared_i \u001B[38;5;241m=\u001B[39m \u001B[43mOLS\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_i\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_noti\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mfit()\u001B[38;5;241m.\u001B[39mrsquared\n\u001B[0;32m    198\u001B[0m vif \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.\u001B[39m \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1.\u001B[39m \u001B[38;5;241m-\u001B[39m r_squared_i)\n\u001B[0;32m    199\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m vif\n",
      "File \u001B[1;32mC:\\GitHub\\WGU.MSDA\\venv\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:922\u001B[0m, in \u001B[0;36mOLS.__init__\u001B[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001B[0m\n\u001B[0;32m    919\u001B[0m     msg \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWeights are not supported in OLS and will be ignored\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    920\u001B[0m            \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn exception will be raised in the next version.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    921\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(msg, ValueWarning)\n\u001B[1;32m--> 922\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mOLS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mendog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmissing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmissing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    923\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mhasconst\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhasconst\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    924\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweights\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_keys:\n\u001B[0;32m    925\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_keys\u001B[38;5;241m.\u001B[39mremove(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweights\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\GitHub\\WGU.MSDA\\venv\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:748\u001B[0m, in \u001B[0;36mWLS.__init__\u001B[1;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001B[0m\n\u001B[0;32m    746\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    747\u001B[0m     weights \u001B[38;5;241m=\u001B[39m weights\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[1;32m--> 748\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mWLS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mendog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmissing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmissing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    749\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mweights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhasconst\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhasconst\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    750\u001B[0m nobs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexog\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    751\u001B[0m weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights\n",
      "File \u001B[1;32mC:\\GitHub\\WGU.MSDA\\venv\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:202\u001B[0m, in \u001B[0;36mRegressionModel.__init__\u001B[1;34m(self, endog, exog, **kwargs)\u001B[0m\n\u001B[0;32m    201\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, endog, exog, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 202\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mRegressionModel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mendog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    203\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpinv_wexog: Float64Array \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_attr\u001B[38;5;241m.\u001B[39mextend([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpinv_wexog\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwendog\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwexog\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mweights\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[1;32mC:\\GitHub\\WGU.MSDA\\venv\\Lib\\site-packages\\statsmodels\\base\\model.py:270\u001B[0m, in \u001B[0;36mLikelihoodModel.__init__\u001B[1;34m(self, endog, exog, **kwargs)\u001B[0m\n\u001B[0;32m    269\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, endog, exog\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 270\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mendog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    271\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minitialize()\n",
      "File \u001B[1;32mC:\\GitHub\\WGU.MSDA\\venv\\Lib\\site-packages\\statsmodels\\base\\model.py:95\u001B[0m, in \u001B[0;36mModel.__init__\u001B[1;34m(self, endog, exog, **kwargs)\u001B[0m\n\u001B[0;32m     93\u001B[0m missing \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmissing\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnone\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     94\u001B[0m hasconst \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhasconst\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m---> 95\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mendog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmissing\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhasconst\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     96\u001B[0m \u001B[43m                              \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     97\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mk_constant \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mk_constant\n\u001B[0;32m     98\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexog \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mexog\n",
      "File \u001B[1;32mC:\\GitHub\\WGU.MSDA\\venv\\Lib\\site-packages\\statsmodels\\base\\model.py:135\u001B[0m, in \u001B[0;36mModel._handle_data\u001B[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001B[0m\n\u001B[0;32m    134\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_handle_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, endog, exog, missing, hasconst, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 135\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43mhandle_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mendog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmissing\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhasconst\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    136\u001B[0m     \u001B[38;5;66;03m# kwargs arrays could have changed, easier to just attach here\u001B[39;00m\n\u001B[0;32m    137\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m kwargs:\n",
      "File \u001B[1;32mC:\\GitHub\\WGU.MSDA\\venv\\Lib\\site-packages\\statsmodels\\base\\data.py:675\u001B[0m, in \u001B[0;36mhandle_data\u001B[1;34m(endog, exog, missing, hasconst, **kwargs)\u001B[0m\n\u001B[0;32m    672\u001B[0m     exog \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(exog)\n\u001B[0;32m    674\u001B[0m klass \u001B[38;5;241m=\u001B[39m handle_data_class_factory(endog, exog)\n\u001B[1;32m--> 675\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mklass\u001B[49m\u001B[43m(\u001B[49m\u001B[43mendog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexog\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmissing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmissing\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhasconst\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhasconst\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    676\u001B[0m \u001B[43m             \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\GitHub\\WGU.MSDA\\venv\\Lib\\site-packages\\statsmodels\\base\\data.py:88\u001B[0m, in \u001B[0;36mModelData.__init__\u001B[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001B[0m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconst_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mk_constant \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m---> 88\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_constant\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhasconst\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     89\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_integrity()\n\u001B[0;32m     90\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cache \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[1;32mC:\\GitHub\\WGU.MSDA\\venv\\Lib\\site-packages\\statsmodels\\base\\data.py:133\u001B[0m, in \u001B[0;36mModelData._handle_constant\u001B[1;34m(self, hasconst)\u001B[0m\n\u001B[0;32m    131\u001B[0m check_implicit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    132\u001B[0m exog_max \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmax(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexog, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m--> 133\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43misfinite\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexog_max\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mall():\n\u001B[0;32m    134\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MissingDataError(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mexog contains inf or nans\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    135\u001B[0m exog_min \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexog, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "copy_of_prepared_medical_data = prepared_medical_data.copy()\n",
    "copy_of_prepared_medical_data.to_csv('./prepared-medical-data-for-feature-selection.csv')\n",
    "\n",
    "dependent_variable = prepared_medical_data['ReAdmis']\n",
    "explanatory_variables = prepared_medical_data.drop(columns=['ReAdmis'])\n",
    "\n",
    "explanatory_variables = pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(explanatory_variables), columns=explanatory_variables.columns)\n",
    "\n",
    "#print(dependent_variable.info())\n",
    "#print(explanatory_variables.info())\n",
    "\n",
    "#We'll take our explanatory variables and run SelectKBest function on them to select the best features for the model based against the dependent variable\n",
    "feature_selection = SelectKBest(f_classif, k='all')\n",
    "feature_selection.fit(explanatory_variables, dependent_variable)\n",
    "\n",
    "feature_column = 'Feature'\n",
    "p_value_column = 'p_values'\n",
    "\n",
    "f_pvalues = { feature_column: explanatory_variables.columns, p_value_column: feature_selection.pvalues_ }\n",
    "feature_selection_pvalues = pd.DataFrame(f_pvalues)\n",
    "feature_selection_pvalues = feature_selection_pvalues.sort_values(p_value_column)\n",
    "\n",
    "kept_features = feature_selection_pvalues[feature_selection_pvalues[p_value_column] < .05]\n",
    "\n",
    "print(kept_features.info())\n",
    "print(kept_features)\n",
    "\n",
    "selected_features_columns = list(kept_features[feature_column])\n",
    "print(selected_features_columns)\n",
    "\n",
    "prepared_medical_data[selected_features_columns].to_csv('./feature-selected-medical-data.csv')\n",
    "\n",
    "# Of the selected features, we are going to check them for `multicollinearity`\n",
    "# We don't want any highly correlated dependent variables as this would cause redundency within the model.\n",
    "multicollinearity_features_to_check = prepared_medical_data[selected_features_columns]\n",
    "\n",
    "\n",
    "vif_data = {\n",
    "    'feature': multicollinearity_features_to_check.columns,\n",
    "    'VIF': [variance_inflation_factor(multicollinearity_features_to_check.values, i) for i in range(len(multicollinearity_features_to_check.columns))]\n",
    "}\n",
    "vif_features = pd.DataFrame(vif_data)\n",
    "print(vif_features)\n",
    "\n",
    "'''\n",
    "    VIF Rules:\n",
    "        - VIF values of 1 are considered correlated\n",
    "        - VIF values below 1 are not correlated\n",
    "        - VIF values greater than 5 are highly correlated and should be reduced to avoid redundancy\n",
    "'''\n",
    "highly_correlated_features = vif_features[vif_features['VIF'] >= 5.0]\n",
    "\n",
    "#Confirm that we have no `multicollinearity`\n",
    "try:\n",
    "    assert (len(highly_correlated_features) == 0), 'No features should have an VIF of 5 or greater.'\n",
    "except AssertionError as exception:\n",
    "    print('High multicollinearity found.', exception)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T17:59:15.245185900Z",
     "start_time": "2023-11-22T17:59:14.600981800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"data-splitting-and-copying\"></a>\n",
    "# D1: Data Splitting, Copy of Split Data\n",
    "\n",
    "Get Training and Test data for `features` and `dependent variable` using train_test_split(...) function.\n",
    "\n",
    "To make sure that the linear regression aspect using the correct value for intercept we will use the [`add_constant()`](https://www.statsmodels.org/stable/generated/statsmodels.tools.tools.add_constant.html)\n",
    "\n",
    "Training and Test data will be split into 80% training and 20% test data for explanatory variables and dependent variable\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_explanatory = sm.add_constant(explanatory_variables[selected_features_columns])\n",
    "y_target = dependent_variable\n",
    "\n",
    "print(X_explanatory.shape)\n",
    "print(y_target.shape)\n",
    "\n",
    "X_explanatory_training, X_explanatory_test, y_target_training, y_target_testing = train_test_split(X_explanatory, y_target, test_size=.2, random_state=42, stratify=y_target)\n",
    "\n",
    "#Save training and test datas\n",
    "X_explanatory_training.to_csv('./x_explanatory-training-dataset.csv')\n",
    "X_explanatory_test.to_csv('./x_explanatory-testing-dataset.csv')\n",
    "\n",
    "y_target_training.to_csv('./y_target-training-dataset.csv')\n",
    "y_target_testing.to_csv('./y_target-testing-dataset.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T17:59:15.243728500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"analysis-description\"></a>\n",
    "# D2: Analysis Description\n",
    "\n",
    "K-nearest Neighbors Classification requires that we pick an appropriate value for *k*. We can use the function [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) from scikit-learn that performs a grid search over a specified hyperparameter space to find the optimal combination of hyperparameters for a given machine learning model.\n",
    "\n",
    "We will visualize our results with Receiver Operating Characteristic (ROC) Curve. This curve is the visual relationship between the *true positive rate (TPR)* and * false positive rate (FPR)* for various thresholds.\n",
    "\n",
    "After the ROC Curve is created we can find the *Area Under the Curve (AUC)* to get a score of how it will work with the classification model.\n",
    "  > AUC is simply the total area under the ROC curve. The larger the value of AUC, the more effective the classifier.\n",
    "  > An AUC of 1 indicates a perfect classifier:\n",
    "  >     If gets all the 1s correctly classified, and it doesn't misclassify any 0s as 1s.\n",
    "  > (Bruce, P., Bruce, A., & Gedeck, P.  2019 p.226)\n",
    "\n",
    "Tools being used for ROC and AUC:\n",
    " * [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    " * [`roc_curve`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)\n",
    " * [`roc_auc_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)\n",
    " * [`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate best value for 'k'\n",
    "param_grid = { 'n_neighbors': np.arange(1, 50) }\n",
    "knn_cross_validation = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
    "knn_cross_validation.fit(X_explanatory_training, y_target_training)\n",
    "\n",
    "n_neighbors = knn_cross_validation.best_params_['n_neighbors']\n",
    "\n",
    "print(f'Best Parameter from KNN Cross-Validation: {n_neighbors}')\n",
    "print(f'Best Score from KNN Cross-Validation: {knn_cross_validation.best_score_}')\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "knn.fit(X_explanatory_training, y_target_training)\n",
    "\n",
    "#use the test data to make some predictions for our Target(Dependent) Variable\n",
    "y_target_predictions = knn.predict(X_explanatory_test)\n",
    "\n",
    "#Get values to evaluate the performance of a classification model\n",
    "true_negative, false_positive, false_negative, true_positive = confusion_matrix(y_target_testing, y_target_predictions).ravel()\n",
    "\n",
    "c_matrix = f'''| {' ':22} | {'Predicted: No Readmission':^5} | {'Predicted: Readmission':^5} |\\n\n",
    "        |{'':->24}|{'':->27}|{'':->24}|\\n\n",
    "        | {'Actual: No Readmission':^5} | {true_negative:^25} | {false_positive:^22} |\\n\n",
    "        | {'Actual: Readmission':22} | {false_negative:^25} | {true_positive:^22} |\\n\n",
    "        |{'':->24}|{'':->27}|{'':->24}|\\n'''\n",
    "\n",
    "display_confusion_matrix = textwrap.fill(c_matrix, width=80)\n",
    "print('\\n')\n",
    "print(display_confusion_matrix)\n",
    "print('\\n')\n",
    "print(f'KNN Classification Scores:')\n",
    "print(f'\\tTraining accuracy: [{knn.score(X_explanatory_training, y_target_training)}]')\n",
    "print(f'\\tTesting accuracy: [{knn.score(X_explanatory_test, y_target_testing)}]')\n",
    "\n",
    "# Plotting ROC\n",
    "y_target_prediction_probability = knn.predict_proba(X_explanatory_test)[:,1]\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_target_testing, y_target_prediction_probability)\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.plot(false_positive_rate, true_positive_rate, label='K-Nearest Neighbor(KNN)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('K-Nearest Neighbor(KNN) ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "rocc_auc_score = roc_auc_score(y_target_testing, y_target_prediction_probability)\n",
    "print(f'AUC (Area Under the Curve) Score: {rocc_auc_score}')\n",
    "\n",
    "report = classification_report(y_target_testing, y_target_predictions)\n",
    "print(report)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T17:59:15.245185900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"classification-analysis-code\"></a>\n",
    "# D3: Classification Analysis Code\n",
    "\n",
    "Because this whole assignment was done in a Notebook the code for the Analysis was created and run in Sections D1 & D2. As such it will be displayed below as Markdown as to not have the code run multiple times in the notebook.\n",
    "\n",
    "###### D1 sections code:\n",
    "\n",
    "```python\n",
    "X_explanatory = sm.add_constant(explanatory_variables[selected_features_columns])\n",
    "y_target = dependent_variable\n",
    "\n",
    "print(X_explanatory.shape)\n",
    "print(y_target.shape)\n",
    "\n",
    "X_explanatory_training, X_explanatory_test, y_target_training, y_target_testing = train_test_split(X_explanatory, y_target, test_size=.2, random_state=42, stratify=y_target)\n",
    "\n",
    "#Save training and test datas\n",
    "X_explanatory_training.to_csv('./x_explanatory-training-dataset.csv')\n",
    "X_explanatory_test.to_csv('./x_explanatory-testing-dataset.csv')\n",
    "\n",
    "y_target_training.to_csv('./y_target-training-dataset.csv')\n",
    "y_target_testing.to_csv('./y_target-testing-dataset.csv')\n",
    "```\n",
    "\n",
    "##### D2 sections code:\n",
    "\n",
    "```python\n",
    "# Calculate best value for 'k'\n",
    "param_grid = { 'n_neighbors': np.arange(1, 50) }\n",
    "knn_cross_validation = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
    "knn_cross_validation.fit(X_explanatory_training, y_target_training)\n",
    "\n",
    "n_neighbors = knn_cross_validation.best_params_['n_neighbors']\n",
    "\n",
    "print(f'Best Parameter from KNN Cross-Validation: {n_neighbors}')\n",
    "print(f'Best Score from KNN Cross-Validation: {knn_cross_validation.best_score_}')\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "knn.fit(X_explanatory_training, y_target_training)\n",
    "\n",
    "#use the test data to make some predictions for our Target(Dependent) Variable\n",
    "y_target_predictions = knn.predict(X_explanatory_test)\n",
    "\n",
    "#Get values to evaluate the performance of a classification model\n",
    "true_negative, false_positive, false_negative, true_positive = confusion_matrix(y_target_testing, y_target_predictions).ravel()\n",
    "\n",
    "c_matrix = f'''| {' ':22} | {'Predicted: No Readmission':^5} | {'Predicted: Readmission':^5} |\\n\n",
    "        |{'':->24}|{'':->27}|{'':->24}|\\n\n",
    "        | {'Actual: No Readmission':^5} | {true_negative:^25} | {false_positive:^22} |\\n\n",
    "        | {'Actual: Readmission':22} | {false_negative:^25} | {true_positive:^22} |\\n\n",
    "        |{'':->24}|{'':->27}|{'':->24}|\\n'''\n",
    "\n",
    "display_confusion_matrix = textwrap.fill(c_matrix, width=80)\n",
    "print('\\n')\n",
    "print(display_confusion_matrix)\n",
    "print('\\n')\n",
    "print(f'KNN Classification Scores:')\n",
    "print(f'\\tTraining accuracy: [{knn.score(X_explanatory_training, y_target_training)}]')\n",
    "print(f'\\tTesting accuracy: [{knn.score(X_explanatory_test, y_target_testing)}]')\n",
    "\n",
    "# Plotting ROC\n",
    "y_target_prediction_probability = knn.predict_proba(X_explanatory_test)[:,1]\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_target_testing, y_target_prediction_probability)\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.plot(false_positive_rate, true_positive_rate, label='K-Nearest Neighbor(KNN)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('K-Nearest Neighbor(KNN) ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "rocc_auc_score = roc_auc_score(y_target_testing, y_target_prediction_probability)\n",
    "print(f'AUC (Area Under the Curve) Score: {rocc_auc_score}')\n",
    "\n",
    "report = classification_report(y_target_testing, y_target_predictions)\n",
    "print(report)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"accuracy-of-classification-model\"></a>\n",
    "# E1: Accuracy of Classification Model\n",
    "\n",
    "In Task 2 of D209 the reduced logistical regression model ended up with an accuracy of *0.98 (98%)* with a MSE of *0.017 (1.7%)* with regard to factors that lead to hospital readmission's. This meant that the model had a high-level of accuracy and the MSE confirmed that the accuracy wasn't inaccurately describing its accuracy as the MSE was really low. When working through D209 Task 1 classification analysis I wanted to use the same question and see if k-nearest neighbor(KNN) could top or at a minimum maintain the accuracy for readmission's modeling.\n",
    "\n",
    "The classification model k-nearest neighbors (KNN) had pretty high accuracy for both the `training` and `test` data. Those accuracies were *0.98 (98%)* for the training dataset and *0.97 (97%)* for testing dataset with regard to which factors affect hospitals' readmission. The point of the *ROC Curve* is to see how far away from the diagonal line we can get in the positive direction. This is because the diagonal line represents the 50% correct classification rate, where complete randomness would hover around this line. If the classification line falls below the diagonal line, it's a poor model, and if it goes above the diagonal line, it's a better model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"model-results\"></a>\n",
    "# E2: Model Results\n",
    "\n",
    "The `KNN (k-nearest neighbor)` model had a remarkable `AUC (Area Under the Curve)` coming in at nearly 100% (*0.9985671147728692*.) This model is a very accurate one when it comes to predicting re-admissions to the hospital. The KNN model almost couldn't be any more accurate. This is on par with the accuracy of the `logistical regression` model performed in *D208*. This model will be extremely helpful in predicting if a patient is likely to be readmitted to the hospital or not as it has reduced the features of the model to the most important ones via the `SelectKBest` functionality."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"classification-limitations\"></a>\n",
    "# E3: Classification Limitations\n",
    "\n",
    "k-nearest neighbor (KNN) limitations:\n",
    "  - Dimensionaility issues:\n",
    "    - KNN is sensitive to too many dimensions/features. Because it calculates the distance between the nearest neighbor and increased feature/dimensions reduces density it highly affects the model.\n",
    "  - Complexity Computational Issue:\n",
    "    - Because every instance has to be calculated, larger datasets can be computationally expensive.\n",
    "  - Imbalanced data:\n",
    "    - If a class is way too big, it may dominate the nearest neighbors causing poor classifications.\n",
    "  - Finding the optimal value of k:\n",
    "    -   Inappropriate k values can lead to poor classifications and to find the right one can simply be trial and error\n",
    "  - Outliers:\n",
    "    - Outliers can affect the distances between instances throwing off the calculations\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"recommended-action\"></a>\n",
    "# E4: Recommended Action\n",
    "\n",
    "In D208, the logistical regression was pretty accurate and reduced down to nine features. During Task 1 of D209 we used *KNN* to attempt to create a better model. The model was reduced down to the three most correlated features. Those features were `Initial_days`, `Children`, and `initial_admission_emergency`.\n",
    "\n",
    "Recommendations include:\n",
    "\n",
    "Enhancing discharge planning is critical in reducing readmission rates, especially for patients with multiple children and those initially admitted through emergency departments. Research indicates that the length of the first hospitalization significantly affects the likelihood of future readmission's. As such, hospitals should focus on improving their discharge planning procedures to ensure patients receive adequate preparation to manage their conditions at home. This may entail providing clear guidance on medication administration, follow-up visits, and identifying warning signs of potential complications. By improving these processes, hospitals can equip patients and their families with the necessary resources to effectively manage their health outside the hospital and ultimately reduce readmission rates, particularly for patients with multiple children and emergency department admissions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(selected_features_columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T17:59:15.246192300Z",
     "start_time": "2023-11-22T17:59:15.246192300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"panopto-recording\"></a>\n",
    "# F: Panopto Recording\n",
    "\n",
    "Summary of Environments:\n",
    "  * OS: Windows 11 + macOS Ventura (I work in both environments)\n",
    "  * Language: Python\n",
    "  * Environment: Jupyter Notebook through JetBrains DataSpell IDE (Cross-Platform)\n",
    "\n",
    "[D209 - Panopto Recording](https://wgu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=89926e14-d13d-4982-a945-afe90061618a)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"code-references\"></a>\n",
    "# G: Code References\n",
    "\n",
    "[Dr. Elleh Webinar - D209 T1 Jul 12 2022](https://wgu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=b73b6274-ef01-4d1b-a59f-aed100228a93)\n",
    " * Used for:\n",
    "     * D1: Splitting Data\n",
    "\n",
    "[Dr. Elleh Webinar - D209 Task 1 KNN Python Code Demo Sep 11 2022](https://wgu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=22e0a080-84f3-4978-a853-af16011c6ce3)\n",
    " * Used for:\n",
    "     * C3: SelectKBest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"source-references\"></a>\n",
    "# H: Source References\n",
    "\n",
    "#### Citations\n",
    " * Bruce, P., Bruce, A., & Gedeck, P. (2019). Practical Statistics for Data Scientists: 50 Essential Concepts. O'Reilly Media, Inc.\n",
    " * Tavva, R. (2020, October 10). K Nearest Neighbour For Supervised Learning. Data Science Blog. [https://data-science-blog.com/blog/2020/10/10/k-nearest-neighbour-for-supervised-learning/](https://data-science-blog.com/blog/2020/10/10/k-nearest-neighbour-for-supervised-learning/)\n",
    " * Cover, T., & Hart, P. (1967). Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13(1), 21-27. Retrieved from [https://ieeexplore.ieee.org/document/1054010](https://ieeexplore.ieee.org/document/1054010)\n",
    " * Dasarathy, B. V. (1991). Nearest neighbor (NN) norms: NN pattern classification techniques. IEEE Computer Society Press. Retrieved from [https://ieeexplore.ieee.org/document/134048](https://ieeexplore.ieee.org/document/134048)\n",
    " * Aggarwal, C. C., Hinneburg, A., & Keim, D. A. (2001). On the surprising behavior of distance metrics in high dimensional space. International Conference on Database Theory, 420-434. Retrieved from [https://link.springer.com/chapter/10.1007/3-540-44503-X_25](https://link.springer.com/chapter/10.1007/3-540-44503-X_25)\n",
    " * Citation: Batista, G. E., Prati, R. C., & Monard, M. C. (2004). A study of the behavior of several methods for balancing machine learning training data. ACM SIGKDD Explorations Newsletter, 6(1), 20-29. Retrieved from [https://dl.acm.org/doi/10.1145/1007730.1007735](https://dl.acm.org/doi/10.1145/1007730.1007735)\n",
    " * Wang, Y., & Wong, K. C. (2008). Enhancing K-nearest neighbor classifier with local information. International Journal of Pattern Recognition and Artificial Intelligence, 22(01), 135-154. Retrieved from [https://www.worldscientific.com/doi/abs/10.1142/S0218001408006325](https://www.worldscientific.com/doi/abs/10.1142/S0218001408006325)\n",
    "  * Vasudev. (2017). What is One Hot Encoding? Why and When Do You Have to Use it? HackerNoon. [https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
